---
title: Assessing replication rates in journals of experimental linguistics
author:
  - name: Kristina Kobrock
    email: kkobrock@uni-osnabrueck.de
    affiliation: University of Osnabrück
    footnote: 1
  - name: Timo B. Roettger
    #email: bob@example.com
    affiliation: Universitetet i Oslo
address:
  - code: University of Osnabrück
    address: Institute of Cognitive Science, Wachsbleiche 27, 49090 Osnabrück
  - code: Universitetet i Oslo
    address: Department of Linguistics and Scandinavian Studies
footnote:
  - code: 1
    text: "Corresponding Author"
abstract: |
  This is the abstract. ~150 words, avoid references, optional graphical abstract, keywords (max. 6, avoid abbreviations, AE spelling)

  It consists of two paragraphs. 

journal: "Journal of Memory and Language"
date: "`r Sys.Date()`"
bibliography: r-references.bib
#linenumbers: true
#numbersections: true
csl: elsevier-harvard.csl
output: rticles::elsevier_article

---

```{r setup, include=FALSE, warning=FALSE, echo=FALSE}
 
knitr::opts_chunk$set(echo = FALSE,
                      cache = TRUE, 
                      warning = FALSE,
                      message = FALSE)

# nifty code using the pacman package
# it checks if the packages specified below are installed, if not, they will be installed, if yes, they will be loaded
if (!require("pacman")) install.packages("pacman")
pacman::p_load(rstudioapi, truncnorm, tidyverse, brms, ggdist, distributional, 
               ggstream, tidybayes, bayesplot, rticles, knitr, ggalluvial)

# set the current working directory to the one where this file is
 #current_working_dir <- dirname(rstudioapi::getActiveDocumentContext()$path) KK: not working when knitting
 current_working_dir <- getwd()
 setwd(current_working_dir)

## global color scheme / non-optimized
### purple
col_purple = "#7b3294"
### light purple
col_lightPurple = "#c2a5cf"
### green
col_green = "#008837"
### light green
col_lightGreen = "#a6dba0"

# loading the data
mention <-  read.csv("../../data/mention.csv")
guidelines  <-  read.csv("../../data/guidelines.csv")
coded_articles  <-  read.csv("../../data/Coding_Articles.csv", sep="|")  

# load models
mention_model <- readRDS(file = "../../data/repl_mention_mdl.RDS")

# merge dfs
df <- full_join(mention, guidelines)

# delete older journals that have been renamed
df <- df %>% 
  filter(!(journals %in% c("LANGUAGE AND COGNITIVE PROCESSES", "LITERARY AND LINGUISTIC COMPUTING")))

# recode jif
df$jif <- ifelse(df$jif == "not retrievable", NA, as.numeric(as.character(df$jif)))

# NAs would be dropped from model. What to do? Either assume 0 or separate models

# recode open access as binary
df$openaccess <- ifelse(df$openaccess == "DOAJ gold", 1, 0)

```

# Introduction
<!--state the objectives of the work and provide an adequate background, avoiding a detailed literature survey or a summary of the results-->

The replication of results is an integral part of cumulative experimental science [e.g., @campbell_reforms_1969; @rosenthal_replication_1990; @zwaan_making_2018]. 
Yet, various scientific disciplines are currently facing what is often referred to as the "replication crisis", a state of affairs that is characterized by an alarming amount of failed replication attempts [@fidler_reproducibility_2018]. <!--KK: I know you said for the preregistration that we should refrain from these strong words so that we don't scare potential readers and publishers off - but on the other hand these are also buzz words that could lead to our study being found and read, aren't they?-->
Here, we define replications as studies that arrive at the same scientific conclusions as an initial study, collecting new data and completing new analyses by using the same methodology [see @barba_terminologies_2018 for a comprehensive overview of different terminological uses].
Coordinated efforts to replicate published findings have uncovered surprisingly low rates of successful replications in fields such as psychology [@open_science_collaboration_estimating_2015], economics [@camerer_evaluating_2016] and social sciences [@camerer_evaluating_2018]. Researchers from these fields report low replication rates ranging from 0.13\% in the education sciences to 1.6\% in psychology publications [@makel_replications_2012; @makel_replications_2014].

One driving factor of this lack of replicability is an asymmetric incentive system that rewards novel confirmatory findings over direct replications and null results. Replication studies are not very popular because the necessary time and resource investment are not appropriately rewarded in contemporary academic incentive systems [e.g., @koole_rewarding_2012; @nosek_scientific_2012]. Both successful replications [@madden_how_1995] and repeated failures to replicate [e.g., @doyen_behavioral_2012] are rarely published, and even if they are, they usually appear in less prestigious outlets than the original findings. 
These dynamics lead to an abundance of positive findings in the absence of possible conflicting negative evidence [see also e.g. @fanelli_pressures_2010].

Experimental linguistics shares research practices that have been shown to decrease the replicability of findings. 
Thus, there are raising concerns about a similarly low number of replication studies conducted and published in this field [e.g. @marsden_replication_2018; @roettger_toward_2019].
A number of failed replication attempts reported in various subfields of linguistics indicate that these concerns are warranted [e.g. in language comprehension: @papesh_just_2015; predictive processing: @nieuwland_large-scale_2018; among others: @chen_chinese_2007; @stack_failure_2018; @westbury_implicit_2018].
In order to thoroughly understand and be able to address this problem, it is important to assess the number of replication attempts in the field of experimental linguistics and their contributing factors.

<!--TR: need to discuss the Marsden et al paper here -->
One subfield of experimental linguistics, second language research, has already been systematically reviewed. @marsden_replication_2018 coded 67 self-labeled L2 replication studies for 136 characteristics. Their results indicate that for every 400 articles, only one replication study is published translating to a replication rate of about 0.25\%. Their sample did not include any direct replication attempts. Changes made to the initial study were very diverse and numerous. These results motivate to assess replication rates and contributing factors for the more general field of experimental linguistics.

The present study assessed the frequency and typology of replication studies that have been published in a representative sample of experimental linguistic journals from 1988 to 2020. 
Our study aimed at answering two main questions: 
How many direct replications are published in experimental linguistics?
Are there factors that affect the replication rates that are either found at the journal level (e.g. journal policies, open access, journal impact factor, etc.) or at the study level (e.g. composition of authors, investigated language, etc.)?
The study consisted of two analyses: 
First, we assessed the rate of mentioning the term replication (search string: replicat\*) across 100 linguistic journals. 
Second, we categorized the type of replication studies (direct, partial, conceptual) in a subset of twenty journals. We then related their prevalence to factors like the years of publication, and the citations of both initial and replication study. 

# How often do journals mention the term replication?
The key dependent variable of the first part of this study was the rate of replication mention for journals relevant to the field of experimental linguistics. 

### Material and methods
<!--provide sufficient details to allow the work to be reproduced by an independent researcher, describe any modifications to existing methods-->
The study design has been preregistered at 2021-03-08 and can be inspected [here: https://osf.io/9ceas/](https://osf.io/9ceas/). 

In order to determine the rates of replication mention for individual journals, we drew on a method introduced by @makel_replications_2012. 
First, a sample of 100 journals relevant to the field of experimental linguistics has been identified by making use of the search engine ["Web of Science" (https://webofknowledge.com)](https://webofknowledge.com) at 2021-03-03. We restricted the search results to journals in the web of science category "Linguistics" which had at least 100 articles published and a high ratio of articles containing the term "experiment\*" in title, abstract or keywords.<!--TR: We might need to be more specific and add the full procedure either here or in an appendix or refer to prereg link--> All articles categorized as written in English between 1945-2020 were taken into account. We selected a subset of those 100 journals that had the highest ratio of mentioning the term "experiment\*" in their articles. This was done to make sure that the subset comprised journals that are relevant for experimental linguistics research. 
```{r exp_ratio}
exp_ratio_median = round(median(df$exp_ratio),1) # 11.5
exp_ratio_min = round(min(df$exp_ratio),1) # 6.1
exp_ratio_max = round(max(df$exp_ratio),1) # 60.3

```
The ratio between overall number of articles and those articles mentioning the term "experiment\*" ranged between `r exp_ratio_min` and `r exp_ratio_max` (with a median of `r exp_ratio_median`). Two journals, namely "Language and Cognitive Processes" and "Language, Cognition and Neuroscience" had to be excluded because it turned out during analysis that both journals have been renamed in 2013 and that they have already been included in our sample under the new name. Our final sample thus included only 98 journals.
The full sample of journals can be inspected [here: https://osf.io/q2e9k/](https://osf.io/q2e9k/) or in the appendix of this article. <!--KK: include in appendix?--> <!--TR: Yes one pretty table!-->
Albeit a rough proxy, this procedure helped us identifying journals relevant to the field of experimental linguistics.

After journal selection, we obtained the total count of articles containing the search term "replicat\*" in title, abstract or keywords for each journal via Web of Science search. 
Following the method presented by Makel et al. [-@makel_replications_2012], the rates of replication mention are calculated by dividing the number of articles containing the term "replicat\*" by the total number of articles for each journal. As we were only interested in experimental linguistic studies, we only included articles containing the search term "experiment\*" in this formula.

Replication mentions were then related to three journal properties: Journal policies with regards to replication studies, journal impact factor and whether the journal is an open access publisher or not.
To gain an understanding of the journal policies with regards to replication studies, we examined the journals' submission guidelines adopting a method suggested by Martin and Clarke [-@martin_are_2017]. 
They grouped psychology journals into four categories dependent on whether they (explicitly or implicitly) encourage replication studies or not in their "instructions to authors" and "aims and scope" sections on the journal websites. For our analysis, we only distinguished between those journals explicitly encouraging replication studies and those that do not. <!-- KK: state reason?--> 
We extracted journal impact factors via Journal Citation Reports (https://jcr.clarivate.com).^[The 2019 journal impact factors are calculated by dividing the citations in 2019 to items published in 2017 and 2018 by the total number of citable items in 2017 and 2018.]
Whether journals offered an open access publication or not was assessed via Web of Science. 
We distinguished between three categories: journals which are listed in the Directory of Open Access Journals (DOAJ) ("DOAJ gold"), journals with some articles being published as open access articles ("partial") and journals with no option to publish open access ("no"). We grouped journals with partial or no option to publish open access together for analysis resulting in a binary categorization of open access.

### Results
<!--clear and concise, include tables and figures-->

```{r number_articles}
sum_articles = sum(df$no_ling) # 51272
sum_exp = sum(df$no_exp) # 8006
sum_repl = sum(df$no_replic) # 347
mention_rate = round((sum_repl / sum_exp), 3)#
```

Out of the `r sum_articles` articles in our sample, `r sum_exp` mentioned the term 'experiment\*' in title, abstract, or keywords and were thus treated as articles presenting an experimental investigation. 
Out of these articles, `r sum_repl` contained the term "replicat\*" suggesting `r round(mention_rate * 100, 2)`\% of all experimental articles to be replication studies. 

```{r mean and variance, echo=FALSE}
mean_rate = round(mean(df$replic_rate)*100 , 1) # ~ 2.7%
median_rate = round(median(df$replic_rate) *100, 1) # ~ 1.6%

min_rate = round(min(df$replic_rate) *100, 1) #  0%
max_rate = round(max(df$replic_rate) *100, 2) # ~ 12.8%
std_rate = round(sd(df$replic_rate) *100, 1) # ~ 3.3%
```

```{r rep rates of 0, echo=FALSE}
zero_mention = nrow(df[df$replic_rate == 0,])
nonzero_mention = nrow(df[df$replic_rate != 0,])
# 43 of 100 journals have rates of replication mention = 0
encouraged = nrow(df[df$binary_policy == 1,])
openaccess = nrow(df[df$openaccess == 1,])
```

The distribution of the rate of replication mention substantially varies across journals ranging from `r min_rate` to `r max_rate`\%. 
Overall, almost half of all journals (n = `r zero_mention`) did not mention the term in any of their articles.
The median mention rate across journals is `r median_rate` (SD = `r std_rate`). 
Figure X illustrates the variation across those journals that exhibited at least one mention of the term.

```{r topten_plot,  out.width="100%", fig.align = 'center', fig.width = 5, fig.asp = 1.6, fig.cap = "Variation in rate of replication mention across those journals that exhibited at least one mention of the term."}

# list of shortened journal names
journals_abb <- c("Humor", "Lang Learn Dev", "Morphology", "Transl Interpreting", "J Logic Lang Inf", "J Mem Lang", "J East Asian Ling", "Cogn Linguist", "Int J Speech Lang La", "J Spec Transl", "Stud Second Lang Acq", "Lang Cogn Neurosci", "Glossa", "Linguistic Res", "First Lang", "J Neurolinguistics", "Lang Learn Technol", "Biling-Lang Cogn", "J Psycholinguist Res", "3L Lang Linguist Literat", "Lab Phonol", "Lang Teach Res", "Lang Cogn", "Lang Linguist Compass", "Metaphor Symb", "Lang Learn", "J Child Lang", "J Semant", "Aphasiology", "J Lang Soc Psychol", "Int J Lang Comm Dis", "J Phon", "Appl Psycholinguist", "Lang Speech", "Acta Linguist Hung", "J Cogn Sci", "Brain Lang", "Recall", "Int J Biling", "Phonetica", "Mind Lang", "Linguist Approach Bi", "IRAL-Int Rev Appl Li", "J Speech Lang Hear R", "Interact Stud", "Arab World Eng J", "Am J Speech-Lang Pat", "Ment Lex", "Clin Linguist Phonet", "Lang Acquis", "System", "Second Lang Res", "Nat Lang Eng", "Comput Assist Lang L", "Lingua", "Lect Notes Comput Sci", "Comput Linguist", "Lect Notes Artif Int", "Phonology", "Interpreting", "Eurasian J Appl Linguist", "J Lang Educat", "Linguistics Vanguard", "Proces de Leng Nat", "Appl Linguist Res J", "Nat Lang Semant", "J Quant Linguist", "Corpus Linguist Ling", "Rev Cogn Linguist", "Interpret Transl Tra", "Poz Stud Contemp Lin", "Pragmat Cogn", "Syntax-UK", "J Res Appl Linguist", "Digit Scholarsh Hum", "Probus", "Innov Lang Learn Teach", "Int J Eng Linguist", "Across Lang Cult", "Rev Roum Linguist", "Intercult Pragmat", "Child Lang Teach The", "Lang Aware", "Gesture", "J Int Phon Assoc", "Metaphor Symb Act", "Iberica", "Annu Rev Appl Linguist", "Ling Antverp New Ser", "Terminology", "Annu Rev Linguist", "J Fr Lang Stud", "Lang Linguist", "Nord J Linguist", "Lang Lit", "Babel-Amsterdam", "Int J Corpus Linguist", "Int J Appl Linguist")

df$journals_abb <- factor(journals_abb)

topten <- 
  df %>% arrange(desc(replic_rate)) %>%
  select(journals_abb, exp_ratio, replic_rate) %>% 
  top_n(nonzero_mention, replic_rate) %>% 
  #mutate(journals = fct_reorder(journals, replic_rate)) %>%
  mutate(journals_abb = fct_reorder(journals_abb, replic_rate)) %>% 
  ggplot(aes(x = replic_rate * 100, 
             y = journals_abb,
             color = replic_rate * 100)) +
  geom_segment(aes(x = 0, 
                   xend = replic_rate * 100,
                   y = journals_abb,
                   yend = journals_abb)) +
  geom_point(size = 3) + 
  scale_color_gradient(low = col_purple,
                       high = col_green)+
 # geom_segment()
  #scale_y_discrete(label = function(x) stringr::str_trunc(x, 20))+
  xlim(0, 15) +
  labs(y = " ",
       x = "\nProportion of 'replicat*' mentions in %\n") +
  theme_minimal() +
  theme(legend.position = "none",
              legend.key.height = unit(2,"line"),
              legend.title = element_text(face = "bold", size = 12),
              legend.text = element_text(size = 12),
              strip.background = element_blank(),
              strip.text = element_text(size = 12, face = "bold"),
              panel.spacing = unit(2, "lines"),
              panel.border = element_blank(),
              plot.background = element_rect(fill = "transparent", colour = NA),
              strip.text.y = element_text(size = 12, hjust = 0),
              axis.text.x = element_text(size = 12),
              axis.text.y = element_text(size = 8),
              axis.line = element_blank(),
              axis.title = element_text(size = 12, face = "bold"),
              plot.title = element_text(size = 14, face = "bold"),
              plot.margin = unit(c(0.6,0.6,0.6,0.6),"cm"))

topten

```

Following preregistered protocol, we statistically estimated the rate of replication mention as predicted relative to the following factors: 
journal impact factors (continuous), open access (binary: open access journal or not), and replication policies (binary: either explicitly encourage or not). 
We used Bayesian parameter estimation based on generalized linear regression models with a binomial link function. 
The model was fitted to the proportion of replication mentions per journal using the R package brms [@burkner_brms_2016]. 
We used weakly informative normal priors centered on 0 (sd = 2.5) for the intercept and Cauchy priors centered on zero (scale = 2.5) for all population-level regression coefficients. 
These priors are what is referred to as regularizing [@gelman_weakly_2008], i.e. our prior assumption is agnostic as to whether the predictors affect the dependent variable, thus making our model conservative with regards to the predictors under investigation. 
Four sampling chains with 2000 iterations each have been run for each model, with a warm-up period of 1000 iterations.
For relevant predictor levels and contrasts between predictor levels, we report the posterior probability for the rate of replication mention. 
We summarize these distributions by reporting the posterior mean and the 95\% credible intervals (calculated as the highest posterior density interval). 


```{r mention_model_prep, echo=FALSE}

# extract coefficients
mention_intercept_est <- round(plogis(fixef(mention_model)[1,1]) * 100, 1)
mention_intercept_lb <- round(plogis(fixef(mention_model)[1,3]) * 100, 1)
mention_intercept_hb <- round(plogis(fixef(mention_model)[1,4]) * 100, 1)

mention_jif_est <- round(fixef(mention_model)[2,1],2)
mention_jif_lb <- round(fixef(mention_model)[2,3],2)
mention_jif_hb <- round(fixef(mention_model)[2,4],2)

mention_oa_est <- round(fixef(mention_model)[3,1],2)
mention_oa_lb <- round(fixef(mention_model)[3,3],2)
mention_oa_hb <- round(fixef(mention_model)[3,4],2)

mention_poli_est <- round(fixef(mention_model)[4,1],2)
mention_poli_lb <- round(fixef(mention_model)[4,3],2)
mention_poli_hb <- round(fixef(mention_model)[4,4],2)

## extract predicted values for JIF
predicted_values <- mention_model %>%
  spread_draws(b_Intercept, b_jif) %>%
  ### make a list of relevant value range of logRT
  mutate(jif = list(seq(0, 4, 0.1))) %>% 
  unnest(jif) %>%
  ### transform into proportion space using the plogis function
  mutate(pred = plogis(b_Intercept + b_jif*jif)) %>%
  pivot_longer(cols = pred,
               names_to = "factors") %>% 
  group_by(jif) %>%
  summarise(pred_m = mean(value, na.rm = TRUE),
            pred_low = quantile(value, prob = 0.025),
            pred_high = quantile(value, prob = 0.975)) 

```

The model estimates the proportion of replication mentions as `r mention_intercept_est`\% [`r mention_intercept_lb`, `r mention_intercept_hb`] at a journal impact factor of 0 and estimates an increase of the proportion with each integer unit of journal impact factor (log odds = `r mention_jif_est` [`r mention_jif_lb`, `r mention_jif_hb`]). Figure X illustrates this relationship. 

```{r correlation_jif_exp_ratio, echo=FALSE}
cor_jif_exp <- round(cor(df$exp_ratio, df$jif, use = "complete.obs", method = c("spearman")),2)
```

However, further explorations indicate that journal impact factor is highly correlated with the number of experimental studies reported in a journal (Spearman correlation = `r cor_jif_exp`). 
Given this correlation, it remains unclear if the term "replicat\*" is used more often in high impact journals or simply more common in journals that generally publish more experimental studies. 

```{r plot_mention_jif, out.width="100%", fig.align = 'center', fig.cap = "Rate of mentioning the term 'replicat*' across sampled journals plotted against their journal impact factor. Each point represents one journal. Point size indicates the proportion of papers categorized as experimental (i.e. larger points indicate journals with more experimental articles). Line and shading indicate model predictions and 95\\% credible intervals."}

## plot predicted values against data
mention_jif <- 
ggplot(data = predicted_values, 
       aes(x = jif, 
           y = pred_m)) +
  geom_ribbon(aes(ymin = pred_low, 
                  ymax = pred_high), 
              alpha = 0.4,
              fill = "grey") +
  geom_line(color = "black", 
            size = 1.5) +
  geom_point(data = df, 
             aes(x = jif, 
                 y = replic_rate,
                 size = exp_ratio,
                 fill = replic_rate),
             pch = 21,
             alpha = 0.7, 
             color = "white") +
  scale_fill_gradient(low = col_purple,
                       high = col_green) +
  ylab("Predicted rate of replication mention") +
  ylim(0, 0.20) +
  xlim(0, 4) +
  labs(title = "Rate of mentioning the term 'replicat*'",
       subtitle = "   ",
       y = "Predicted rate of replication mention\n",
       x = "\nJournal Impact Factor") +
  theme_minimal() +
  theme(legend.position = "none",
              legend.key.height = unit(2,"line"),
              legend.title = element_text(face = "bold", size = 12),
              legend.text = element_text(size = 12),
              strip.background = element_blank(),
              strip.text = element_text(size = 12, face = "bold"),
              panel.spacing = unit(2, "lines"),
              panel.border = element_blank(),
              plot.background = element_rect(fill = "transparent", colour = NA),
              strip.text.y = element_text(size = 12, hjust = 0),
              axis.text = element_text(size = 12),
              axis.line = element_blank(),
              axis.title = element_text(size = 12, face = "bold"),
              plot.title = element_text(size = 14, face = "bold"),
              plot.margin = unit(c(0.6,0.6,0.6,0.6),"cm"))

mention_jif
```

The model estimates the impact of whether the journal allows for open access publishing or not and whether replications are explicitly encouraged or not both as positive, i.e. the term replication is mentioned more often in both open access journals and  journals that explicitly encourage direct replications. However, the uncertainty around these estimates is substantial (open access: `r mention_oa_est` [`r mention_oa_lb`, `r mention_oa_hb`]; policy: `r mention_poli_est` [`r mention_poli_lb`, `r mention_poli_hb`]) although not surprising given the small number of journals that explicitly encourage direct replications (`r encouraged` out of 98), and the small number of open access journals (`r openaccess` out of 98).

```{r rate of replication mention, echo=FALSE, eval=FALSE}

mention_by_exp_ratio <- df %>% 
  select(journals, exp_ratio, replic_rate) %>% 
  arrange(desc(exp_ratio)) %>% 
  mutate(
    Journals = factor(journals, ordered=T),
    "Ratio of experimental studies" = exp_ratio,
    "Rate of replication mention" = round(replic_rate*100, 2)
  ) %>% 
  select(Journals, "Ratio of experimental studies", "Rate of replication mention")
head(mention_by_exp_ratio, 10)


mention_by_rate <- df %>% 
  select(journals, replic_rate) %>% 
  arrange(desc(replic_rate)) %>% 
  mutate(
    Journals = journals,
    "Rate of replication mention" = round(replic_rate*100, 2) 
  ) %>% 
  select(Journals, "Rate of replication mention")
head(mention_by_rate, 1)
```

```{r policy, echo=FALSE, eval=FALSE}
no_encourage <- df %>% 
  filter(binary_policy == 1) %>% 
  count() 
# --> only 2 out of 100 journals explicitly encourage submission of replication studies
```

### Discussion
The articles of almost half of all journals in our sample (n = `r zero_mention`) did not mention the term 'replicat\*' at all. The maximum rate of replication mention is `r max_rate`\%, but the median mention rate lies substantially lower at `r median_rate` (SD = `r std_rate`). This rate is comparable to the 1.6\% that @makel_replications_2012 have reported in their study on psychology journals.

Our preregistered model estimates an increase in proportion of mentioning the term 'replicat*' with increasing journal impact factor. But according to further explorations, this effect is likely due to the higher number of experimental studies published in journals with higher impact factor. This correlation makes sense in our sample because experimental linguistic articles are often associated with psychology adjacent fields, possibly attracting a broader audience and being cited more widely than articles belonging to other subfields of linguistics.
Given this correlation, we cannot draw any conclusions on the role of journal impact factors with regards to publishing replication studies.

The role of open access publishing options and journal policies is clearer: According to our model, the term 'replicat*' is mentioned more often in both open access journals and journals that explicitly encourage direct replications on their websites. Although the uncertainty around these estimates is substantial, we can draw the tentative conclusion that journals which support open science by offering open access publishing and by explicitly encouraging the submission of replication studies, also publish replication studies more often than other journals.

# How many of these mentions are actual replications?
The second part of the study aimed at obtaining a clearer picture about what types of replication studies are published and whether direct replications are becoming more frequent over time.
Because the term "replication" is commonly used in ambiguous ways, the articles that contained the search term "replicat\*" required further analysis to determine whether the articles in question indeed reported a replication study or used the term in a different way. 

### Material and methods
From the superset of 100 journals obtained above, the first 20 journals (i.e. those journals with the highest proportion of experimental studies) were selected for a more detailed analysis while excluding journals for which less than 2 hits (TS=(replicat\*)) could be obtained (see [here](https://osf.io/f3yp8/) for a list of article counts per journal: https://osf.io/f3yp8/). 
Because of the skewed distribution of our sample (114 hits for Journal of Memory and Language, and less than 40 for all other journals), we randomly selected 50 out of the 114 articles for the Journal of Memory and Language in order to achieve a more balanced distribution of papers across journals (see [here](https://osf.io/6vfpe/) for details: https://osf.io/6vfpe/).
The sampling procedure above resulted in 210 possible self-labeled replication studies. 

<!--TR: subset analysis of JML?-->

We identified whether the article in question indeed presented a replication study or not. 
The relevant parts of the papers were title and abstract of the paper, sentences around occurrences of the search term "replicat\*" as well as the paragraph before the Methods section and the first paragraph of the Discussion section (following the procedure specified by Makel et al. [-@makel_replication_2016]).
If the authors explicitly claimed that (one of) their research aim(s) was to replicate the result or methods of an initial study, this article was treated as a replication and was submitted to further analysis according to the preregistered coding scheme which can be inspected [here: https://osf.io/ct2xj/](https://osf.io/ct2xj/). 

When extracting number and types of changes made to the initial study, we assumed that the authors of a replication study did not make any drastic changes *without* reporting them. 
The replication studies were classified according to three types: 
direct replication (0 changes), partial replication (1 change) and conceptual replication (2 or more changes), following Marsden et al. [-@marsden_replication_2018]. 
We noted the nature of the change as one of the following categories (yes/no): 
experimental paradigm, sample, materials/experimental set-up, dependent variable, independent variable, and control. 
We also noted the language under investigation.
The information on whether the article was published open access as well as citation counts and years of publication for both studies were obtained from Web of Science.
An author overlap was attested when at  least one author was a (co-)author on both articles.
When coding the articles, some cases turned up that could not be anticipated when the study was preregistered. When several self-labeled replication studies were mentioned in one article, we chose the first mentioned study for analysis. If there were one independent, but also one or more inner-paper replications, i.e. experiments that replicated previously obtained results from the same article, we chose the independent study for analysis.

### Results
<!--clear and concise, include tables and figures-->

```{r replication_nos, echo=FALSE}

nr_experimental <- coded_articles %>% filter(experimental == "1") %>% count() # 200 are experimental
ratio_experimental <- round((nr_experimental / nrow(coded_articles)) * 100, 1) # 95,2% are experimental
nr_replications <- coded_articles %>% filter(replication == "1" & experimental == "1") %>% count() # 116 are replications
nr_direct <- coded_articles %>% filter(type_replication == "direct") %>% count() # 8 are replications
prop_direct <- round((nr_direct / nr_replications) * 100, 1) # 6.9% of replications are direct
nr_partial <- coded_articles %>% filter(type_replication == "partial") %>% count() # 42
nr_conceptual <- coded_articles %>% filter(type_replication == "conceptual") %>% count() #66

```

Out of the `r nrow(coded_articles)` articles in the subsample, `r nr_experimental` (`r ratio_experimental`\%) indeed presented experimental linguistics research. The remaining `r nrow(coded_articles)-nr_experimental` (`r 100-ratio_experimental`\%) have not been experimental studies, but rather commented and reflected on the field or presented research using computer simulations and modeling techniques. `r nr_replications` were self-claimed replications according to our criteria. The remaining `r nrow(coded_articles)-nr_replications` mentions were articles that mentioned the term in other contexts or studies that did not specify the concrete aim of replicating an initial study's design or results. These articles that did not qualify as replication studies according to our predefined criteria could also include articles which mentioned that their results "replicated" the results of one or more previous studies using the term in a very broad sense. In these cases, it was usually not clear which specific research methods or aims from previous studies should be replicated. We thus decided to handle these cases not as replication studies.
Out of the replication studies we identified, we categorized `r nr_conceptual` (`r round((nr_conceptual/nr_replications)*100,1)`\%) as conceptual, `r nr_partial` (`r round((nr_partial/nr_replications)*100,1)`\%) as partial, and only `r nr_direct` (`r prop_direct`\%) as direct replications.
```{r inner paper, echo=FALSE}
# How many replication studies were inner-paper replications?
reps_inner_paper = coded_articles %>% 
  filter(replication == "1") %>% 
  filter(cit_init_study == "same") %>% 
  count() 
# --> 37
inner_paper = round(((reps_inner_paper / nr_replications) * 100),1)
# --> ~ 31,62 %
```
About one third (`r inner_paper`\%) of the replications were published in the scope of the same paper as the initial study. Publishing multiple experiments within one article and replicating one's own previously obtained results thus seems to be good practice in the field of experimental linguistics.

```{r success, echo=FALSE}

# How many of the direct replications were independent
independent_reps = coded_articles %>% filter(type_replication == "direct") %>% filter(auth_overlap == "0") %>% count() # 3 independent
independent_rate = (independent_reps / nr_direct) * 100
# --> 37.5%

# How many of these were (self-labeled) successful?
success_reps = coded_articles %>% 
  filter(type_replication == "direct",
         auth_overlap == "0",
         success == "1") %>% 
  count() # 5

```

Looking closer at direct replications, `r independent_reps` studies were independent studies, i.e. there was no overlap between authors of the initial study and the replication study. 
Out of these independent direct replication studies, `r success_reps` were self-labeled as successful replications. 
In other words, our sample included only one failed independent and direct replication attempt.

Figure X illustrates the development of replication studies across the time span of our sample. 
While the overall number of studies increased over the years, the proportion of direct replications remained stable at best. 
However, it seems as if there is an increasing number of partial and conceptual replications that was published within the last few years.^[Given the small number of direct replications in our sample, both a descriptive assessment and an inferential assessment as preregistered are very uninformative. The reader is directed to the supplementary materials, if they are interested in the model outputs of the preregistered analysis.]

```{r stream_plot, out.width="100%", fig.align = 'center', fig.asp = 0.7, fig.cap = "Development of amount of replication studies published over time."}

# subset data
replication_sub <- coded_articles %>% 
  filter(experimental == 1)

### let's plot the distribution in stream plot
replication_sub_agg <- replication_sub %>% 
  mutate(type2 = ifelse(is.na(type_replication), "no", 
                        ifelse(type_replication == "", "no", type_replication))) %>% 
  group_by(type2, pub_year) %>% 
  summarise(n = n())
  
## change level order           
replication_sub_agg$type2 <- factor(replication_sub_agg$type2, 
                                    levels = c("no", "conceptual", "partial", "direct"))

## stream plot
stream_plot <- 
ggplot(replication_sub_agg, aes(x = pub_year, y = n, fill = type2)) +
  geom_stream(bw = 0.7)  +
  scale_fill_manual(values = c("#998ec3", "#fee0b6", "#f1a340", "#b35806"),
                    name = "") +
  labs(title = "Number of replication types from 1988-2020",
       subtitle = "   ",
       x = "\nYear",
       y = "number of papers in corpus\n") +
  theme_minimal() +
  theme(legend.key.height = unit(2,"line"),
        legend.title = element_text(face = "bold", size = 12),
        legend.text = element_text(size = 12),
        strip.background = element_blank(),
        strip.text = element_text(size = 12, face = "bold"),
        panel.spacing = unit(2, "lines"),
        panel.border = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA),
        strip.text.y = element_text(size = 12, hjust = 0),
        axis.text = element_text(size = 12),
        axis.line = element_blank(),
        axis.title.y = element_text(size = 10),
        axis.title.x = element_text(size = 12),
        plot.title = element_text(size = 14, face = "bold"),
        plot.margin = unit(c(0.6,0.6,0.6,0.6),"cm"))
stream_plot

```

```{r river plot, echo=FALSE, include=FALSE}

myedges <- data.frame(ID = 1:9,
                   N1 = c("successful", "failure", 
                          "author overlap", "independent",
                          "direct", "conceptual", "partial",
                          #"partial",
                          "No replications", "Self-claimed replications"),
                   Value = c(2,1,
                             5,3,
                             8,66,42,
                             94,116))

myedges$N1 <- paste(myedges$N1, "\n", paste0(myedges$Value))
myedges$N2 <- c(rep("independent \n 3", 2), 
                rep("direct \n 8", 2), 
                rep("Self-claimed replications \n 116", 3), 
                rep(paste("All surveyed papers \n", 210), 2))


mynodes <- data.frame(ID=c(myedges$N1, paste("All surveyed papers \n", 210)), 
                      x = c(1, 1, 2, 2, 3, 3, 3, 4, 4, 5), 
                      y = c(10,14,
                            8, 12, 
                            10, 7.5, 5, 
                            1, 7.5, 
                            5))

# styles
palette <- c("gray", "#AA3377", "gray", 
             "#AA3377", "grey", "gray", 
             "gray", "gray", "#AA3377")

#AA3377

mystyles = lapply(mynodes$y, function(n) {
  list(col = palette[n])
})

for(i in 1:length(palette)){
  mystyles[[i]]$col<-palette[i]
}

names(mystyles) <- mynodes$ID

custom.style <- riverplot::default.style()
custom.style$textcex <- 1.8

svg("../../plots/River.svg", width=10)
myriv <- riverplot::makeRiver(nodes=mynodes, edges=myedges, node_styles=mystyles)
plot(myriv, default_style=custom.style, nodewidth=1, plot_area = 0.95)
dev.off()
plot(myriv, default_style=custom.style, nodewidth=1, plot_area=.88)

```

```{r language rep, echo=FALSE}
# How many replication studies were replications in a different language?
reps_diff_lang = coded_articles %>% 
  filter(replication == "1", experimental == "1") %>% 
  filter(str_detect(comments, "replication in a different language") |
         str_detect(language, "previous") |
         str_detect(language, "initial")) %>%
  count() 
# --> 28
diff_lang = round(((reps_diff_lang / nr_replications) * 100),1)
# --> ~ 24,1 %

# number of English studies across replications
reps_english = coded_articles %>% 
  filter(replication == "1", experimental == "1",
         str_detect(language, "English")) %>% 
  count()  # --> 78
         
reps_english_rate = round(((reps_english / nr_replications) * 100),1) # --> 67,2%

```

One possible reason for the fact that (direct) replication rates are not increasing for the field according to our analysis could be that experimental linguistics predominantly replicates experimental findings across languages, making the replication studies conducted by definition only partial replications.
However, only one quarter of replications targeted a different language than the initial study (`r diff_lang`\%). 
The majority of replication efforts were conducted within the same language as the initial study.
In fact, `r reps_english_rate`\% of all replication studies in our sample had one variety of English as the main language of investigation either in the replication or in the corresponding initial study.

```{r years between, echo=FALSE}
# What is the median number of years between an initial and a replication study?
median_year_between <- coded_articles %>% 
  filter(cit_init_study != "same") %>% # exclude inner-paper replications
  summarise(median_years = round(median(years_between, na.rm = TRUE),1)) # 7 years

```

```{r citation, echo=FALSE}

coded_articles <- coded_articles %>% 
  mutate(year_til_today = 2020 - pub_year)

# What is the mean and median citation count of initial studies before a replication study was published?
mean_citation_between <- coded_articles %>% 
  filter(cit_init_study != "same", # exclude inner-paper replications
         replication == 1, experimental == 1) %>% 
  summarise(mean_cit_init = round(mean(init_cit_til_rep, na.rm = TRUE),1)) # ~ 41.6 citations
median_citation_between <- coded_articles %>% 
  filter(cit_init_study != "same", # exclude inner-paper replications
         replication == 1, experimental == 1) %>% 
  summarise(median_cit_init = round(median(init_cit_til_rep, na.rm = TRUE),1)) # 20 citations

mean_citation_rate <- round(mean_citation_between/median_year_between,1)

# What is the mean and median citation count of replication studies 
median_citation_replication <- coded_articles %>% 
  filter(cit_init_study != "same",
         replication == 1, experimental == 1) %>% 
  summarise(median_cit_rep = round(median(rep_citation, na.rm = TRUE), 1), # 17
            median_cit_rep_rate = round(median(year_til_today / median_cit_rep), 1)) # 0.6

# jif of journals publishing replication and initial study
median_jif <- df %>% 
  summarise(median_jif = round(median(jif, na.rm = TRUE), 1)) # 1.1

```

The median number of years between an initial and a replication study is `r median_year_between` years.
Initial studies were on average `r mean_citation_between` times cited before a replication was published which amounts to an average yearly citation rate of `r mean_citation_rate` citations. 
This average citation rate is well above the impact factor of journals in publishing experimental studies in linguistics (median journal impact factor in superset: `r median_jif`). 
However, replications were on average `r median_citation_replication[1]` times cited which only amounts to an average yearly citation rate (calculated up to the time of analysis) of `r median_citation_replication[2]` citations. 

### Case study of Journal of Memory and Language

```{r jml}
# having a closer look at jml
jml_data <- coded_articles %>% 
  filter(journal == "JOURNAL OF MEMORY AND LANGUAGE")

jml_jif <- df %>% 
  arrange(desc(jif)) %>% 
  head(1) %>% 
  select(jif) %>% 
  round(1)
# --> 3.9

## how many really experimental
jml_exp <- jml_data %>% 
  filter(experimental == "1") 
jml_exp_ratio = count(jml_exp)/count(jml_data) 
# ---> 50 = 100%

## of those how many actual replications
jml_reps <- jml_exp %>% 
  filter(replication == "1") 
jml_rep_ratio = count(jml_reps)/count(jml_exp)
# ---> 34 = 68%

## of those what types of replications
jml_types <- round(xtabs(~type_replication, jml_reps), 2)
jml_types_perc <- round(xtabs(~type_replication, jml_reps) / nrow(jml_reps)*100, 1)
### 0.44 conceptual
### 0.09 direct --> compared to 7% over all journals
### 0.47 partial

## of those author overlap
jml_authoverl <- round(xtabs(~type_replication + auth_overlap, jml_reps), 2)
jml_authoverl_perc <- round(xtabs(~type_replication + auth_overlap, jml_reps) / nrow(jml_reps), 2)
###            no   yes 
### conceptual 0.18 0.26
### direct     0.06 0.03
### partial    0.24 0.24

# all journals:
###            no   yes 
### conceptual 0.26 0.30
### direct     0.03 0.04
### partial    0.15 0.21

jml_success <- jml_data %>% 
  filter(replication == 1, type_replication == "direct", auth_overlap == 0, success == 1) %>% 
  count() # --> 1 successful

## direct independent replication rate 
jml_indreprate <- round(nrow(jml_data[jml_data$replication == 1 &
                                        jml_data$type_replication == "direct" &
                                        jml_data$auth_overlap == 0,]) / nrow(jml_data), 3)
### 0.04 = 4% compared to 0.015 = 1,5%

## direct replication rate 
jml_reprate <- round(nrow(jml_data[jml_data$replication == 1 &
                                        jml_data$type_replication == "direct",]) / nrow(jml_data), 3)
### 0.06 = 6% compared to 6.9%
```
When having a closer look at the Journal of Memory and Language which accounts for the largest part of our sample compared to other journals and is the journal with the highest impact factor (`r jml_jif`), we find that `r count(jml_reps)` (`r (count(jml_reps)/count(jml_exp))*100`\%) of the `r count(jml_exp)` studies in our sample are replication studies. Of these, `r jml_types["conceptual"]` (`r jml_types_perc["conceptual"]`\%) are conceptual, `r jml_types["partial"]` (`r jml_types_perc["partial"]`\%) are partial, and `r jml_types["direct"]` (`r jml_types_perc["direct"]`\%) are direct replication studies which is in line with the results for the whole sample. When excluding for studies with author overlap and only taking into account the independent direct replication studies, `r jml_authoverl[2]` of the studies published in Journal of Memory and Language were independent direct replication studies (one of which was successful). 

Figure X shows the development of publications in the Journal of Memory and Language over time. While there was a peak of direct replication studies in 2003, the numbers decreased again after that year. In 2020 no direct replication studies have been published in this journal at all. However, the proportion of partial replications seems to be increasing over the last few years (although we are talking of very small numbers here).

```{r jml_stream_plot, out.width="100%", fig.align = 'center', fig.cap = "Development of amount of replication studies published over time in the Journal of Memory and Language."}

# subset data
replication_sub <- jml_exp

### let's plot the distribution in stream plot
replication_sub_agg <- replication_sub %>% 
  mutate(type2 = ifelse(is.na(type_replication), "no", 
                        ifelse(type_replication == "", "no", type_replication))) %>% 
  group_by(type2, pub_year) %>% 
  summarise(n = n())
  
## change level order           
replication_sub_agg$type2 <- factor(replication_sub_agg$type2, 
                                    levels = c("no", "conceptual", "partial", "direct"))

## stream plot
stream_plot <- 
ggplot(replication_sub_agg, aes(x = pub_year, y = n, fill = type2)) +
  geom_stream(bw = 0.7)  +
  scale_fill_manual(values = c("#998ec3", "#fee0b6", "#f1a340", "#b35806"),
                    name = "") +
  labs(title = "Number of replication types from 1995-2020",
       subtitle = "   ",
       x = "\nYear",
       y = "number of papers in corpus\n") +
  theme_minimal() +
  theme(legend.key.height = unit(2,"line"),
        legend.title = element_text(face = "bold", size = 12),
        legend.text = element_text(size = 12),
        strip.background = element_blank(),
        strip.text = element_text(size = 12, face = "bold"),
        panel.spacing = unit(2, "lines"),
        panel.border = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA),
        strip.text.y = element_text(size = 12, hjust = 0),
        axis.text = element_text(size = 12),
        axis.line = element_blank(),
        axis.title.y = element_text(size = 10),
        axis.title.x = element_text(size = 12),
        plot.title = element_text(size = 14, face = "bold"),
        plot.margin = unit(c(0.6,0.6,0.6,0.6),"cm"))
stream_plot

```

### Discussion
Only `r (nr_replications / nr_experimental) * 100`\% of the experimental studies in our sample were replication studies according to our criteria.
Out of these, `r nr_conceptual` were categorized as conceptual, `r nr_partial` as partial, and only `r nr_direct` as direct replications which amounts to a direct replication rate of `r prop_direct`\% in our sample. This clearly indicates that replication attempts, and especially direct replication attempts, are very rare in the experimental linguistics literature - a fact that compares to other research fields.

The development of publications over time shows that the overall number of studies increased, but the proportions of direct replication studies remained stable.
However, we can speculate that there is an increasing number of partial and conceptual replications that have been published within the last few years.

Our analysis of the years between the publication of the initial and replication study and the citation counts of both is in line with @marsden_replication_2018 who investigated the prevalence of replication studies across second language research. They found that replication studies were on average conducted after more than six years and after over a hundred citations of the original study and concluded that  replications are either only performed or only published after the original study had already substantially impacted the field. Our findings support this hypothesis from the point of view of experimental linguistics research: Replication studies are published after a median of `r median_year_between` years and after `r mean_citation_between` citations of the initial study.
The average drop in citations for replication studies compared to the initial study is in line with the lack of perceived prestige of replication studies [e.g., @koole_rewarding_2012; @nosek_scientific_2012].

The findings for the journal with the highest impact factor from our sample, Journal of Memory and Language, are comparable to the full sample. The trend over the last few years indicates that partial replication studies are more and more being published, but direct replication studies are unfortunately still seldomly conducted and/or published.

# General discussion
The overall picture our analyses draw for replication attempts in the field of experimental linguistics is that they are far too uncommon. This relates to other fields of research for which a lack of replication studies has been attested [@makel_replications_2012; @makel_replications_2014; @open_science_collaboration_estimating_2015; @camerer_evaluating_2016; @camerer_evaluating_2018]. The median rate of replication mention of `r median_rate` (SD = `r std_rate`)`we find for experimental linguistics is in line with the rates found for psychology, social sciences and economics to name just a few.

The asymmetric incentive system of publication does not reward replication studies and they are also not perceived as being prestiguos [e.g., @koole_rewarding_2012; @nosek_scientific_2012]. This is reflected in the citation counts which are at a median of ... for the replications in our sample and at a median of ... for the initial studies in our sample. The low rates of replications in the field of experimental linguistics might be due to replication studies not being conducted, but they might also be due to replication studies not being *published*.

A reason for this could be that journal guidelines usually do not encourage to submit replication studies. But this policy is related to the amount of replication studies published in a journal. We thus suggest in support of @marsden_replication_2018 and @martin_are_2017 that journals should accept replication studies more often and should also make more explicit and public that they encourage their submission. Journals do not need to fear any negative consequences like a loss of reputation because our results indicate that journals which frequently mention the term 'replicat*' in fact have a proportionally high impact factor. This effect is probably only correlational and not a causation, but still indicates that replication studies are frequently published in journals with high impact factors.
The DOAJ gold standard with respect to open access publishing is also positively related to publishing replication studies. This might be due to journals which support open access are also more open to replication studies, both being important tools for the open science movement.

The development of publications over time does not give much cause for hope: While the total amount of studies published increased over the last few years, the amount of direct replications remained stable at best.
However, we can speculate that there is an increasing number of partial and conceptual replications that have been published within the last few years.

On the other hand the vast amount of reproducibility initiatives founded in the field during the last few years give hope that the situation is likely going to change in the near future. e.g. many babies, many speeches etc. <!--KK: mention covid year 2020?-->

### Caveats
The procedure used here is necessarily only a rough proxy of relevant experimental linguistic articles published in the field and several articles might thus have been overlooked and not been included in the analysis. The articles have been coded by only a single person who was not an expert in this field. <!--KK: We could still do sth. about this.--> 

# Appendices
identified as A, B, etc.
```{r cit-and-years-direct, out.width="100%", fig.align = 'center', fig.asp = 0.7, fig.cap = "Relation between citation counts and years of publication of direct replication studies."}
# preparing data for plot

# wrangling citation data
cit_data <- coded_articles %>% 
  filter(type_replication == "direct") %>% # only include direct replications
  mutate(
    obs_nr = c(1:8)
  ) %>% 
  select(obs_nr, rep_citation, init_cit_til_rep) %>% 
  pivot_longer(cols = c(rep_citation, init_cit_til_rep), values_to = "citation") %>% 
  mutate(
    study_kind = ifelse(name == "rep_citation", "replication", "initial")
  ) %>% 
  select(obs_nr, study_kind, citation)

# wrangling year of publication data
pub_data <- coded_articles %>% 
  filter(type_replication == "direct") %>% # only include direct replications
  mutate(
    obs_nr = c(1:8)
  ) %>%
  select(obs_nr, pub_year, year_init_study) %>% 
  pivot_longer(cols = c(pub_year, year_init_study), values_to = "years") %>% 
  mutate(
    study_kind = ifelse(name == "pub_year", "replication", "initial")
  ) %>% 
  select(obs_nr, study_kind, years)

# creating one data frame for plotting
plot_data <- full_join(cit_data, pub_data)

# creating a plot
plot_data %>% 
  ggplot(aes(x = years, y = citation, color = study_kind, group = obs_nr)) +
  geom_point() +
  geom_line(color = "lightgrey") +
  labs(
    x = "years",
    y = "citations"
    #title = "After how many years and how many citations do replication studies get published?"
  )
```


References {#references .unnumbered}
==========
