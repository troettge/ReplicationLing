---
title: Assessing replication rates in journals of experimental linguistics
author:
  - name: Kristina Kobrock
    email: kkobrock@uni-osnabrueck.de
    affiliation: University of Osnabrück
    footnote: 1
  - name: Timo B. Roettger
    #email: bob@example.com
    affiliation: University of Oslo
address:
  - code: University of Osnabrück
    address: Department, Street, City, State, Zip
  - code: University of Oslo
    address: Department, Street, City, State, Zip
footnote:
  - code: 1
    text: "Corresponding Author"
abstract: |
  This is the abstract. ~150 words, avoid references, optional graphical abstract, keywords (max. 6, avoid abbreviations, AE spelling)

  It consists of two paragraphs. 

journal: "Journal of Memory and Language"
date: "`r Sys.Date()`"
bibliography: r-references.bib
#linenumbers: true
#numbersections: true
csl: elsevier-harvard.csl
output: rticles::elsevier_article
---

```{r setup, include=FALSE, warning=FALSE, echo=FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      cache = TRUE, 
                      warning = FALSE)


# nifty code using the pacman package
# it checks if the packages specified below are installed, if not, they will be installed, if yes, they will be loaded
if (!require("pacman")) install.packages("pacman")
pacman::p_load(rstudioapi, truncnorm, tidyverse, brms, ggdist, distributional, 
               ggstream, tidybayes, bayesplot, rticles)

# set the current working directory to the one where this file is
#current_working_dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
#setwd(current_working_dir)

## global color scheme / non-optimized
### purple
col_purple = "#7b3294"
### light purple
col_lightPurple = "#c2a5cf"
### green
col_green = "#008837"
### light green
col_lightGreen = "#a6dba0"

# loading the data
mention <-  read.csv("../../data/mention.csv")
guidelines  <-  read.csv("../../data/guidelines.csv")
replication  <-  read.csv("../../data/coded.csv")  

# load models
mention_model <- readRDS(file = "../../data/repl_mention_mdl.RDS")

# merge dfs
df <- full_join(mention, guidelines)

# delete older journals that have been renamed
df <- df %>% 
  filter(!(journals %in% c("LANGUAGE AND COGNITIVE PROCESSES", "LITERARY AND LINGUISTIC COMPUTING")))

# recode jif
df$jif <- as.numeric(ifelse(df$jif == "not retrievable", NA, df$jif))

# NAs would be dropped from model. What to do? Either assume 0 or separate models

# recode open access as binary
df$openaccess <- ifelse(df$openaccess == "DOAJ gold", 1, 0)

```

# Introduction
<!--state the objectives of the work and provide an adequate background, avoiding a detailed literature survey or a summary of the results-->

The replication and reproducibility of results is key to good scientific practice. Yet, various scientific disciplines are currently facing what is popularly referred to as a "reproducibility" or "replication crisis" characterized by a  small amount of published replication studies and an increasing number of failed replication attempts [@fidler_reproducibility_2018]. <!--KK: I know you said for the preregistration that we should refrain from these strong words so that we don't scare potential readers and publishers off - but on the other hand these are also buzz words that could lead to our study being found and read, aren't they?--> Researchers from fields such as psychology [@makel_replications_2012], education science [@makel2014facts], and special education research [@makel_replications_2016] have assessed the amount of direct replications in their respective fields and report alarmingly low replication rates ranging from 0.13\% in the education sciences to 1.07\% in psychology publications.
Coordinated efforts to replicate published findings have uncovered surprisingly low rates of successful replications ranging from 47\% in psychology [@open_science_collaboration_estimating_2015] to 61\% in economics [@camerer_economics_2016] and 62\% in the social sciences [@camerer_socscience_2018]. <!--KK: footnote? measure: whether the effect size of the replication study fell wihtin the 95% confidence interval of the original-->
A number of failed replication attempts reported in various subfields of linguistics indicate that the field is not immune to these raising concerns [e.g. in language comprehension: @papesh_just_2015; predictive processing: @nieuwland_large-scale_2018; among others: @chen_chinese_2007; @stack_failure_2018; @westbury_implicit_2018].

Experimental linguistics shares research practices that have been shown to decrease the replicability of findings. Thus, there are raising concerns about a similarly low number of replication studies conducted and published in this field [e.g. @marsden_replication_2018; @roettger2019toward].
One driving factor for this phenomenon is an asymmetric incentive system that rewards novel confirmatory findings more than direct replications and null results. This leads to an abundance of positive findings in the absence of possible conflicting negative evidence [see also e.g. @fanelli_pressures_2010]. In order to thoroughly understand and be able to address this problem, it is important to assess the number of replication attempts and their contributing factors.

In order to evaluate the replication rate in experimental linguistics, the present study assessed the frequency and typology of replication studies that have been published in a representative sample of experimental linguistic journals from their beginnings until 2020. 
The study consisted of two parts: 
First, the frequency of self-reported replication attempts across 100 linguistic journals were assessed and the rate of replication mention was related to factors like journal impact factor, publishing policy and publication access. 
Second, the type of replication studies (direct, partial, conceptual) published in a subset of 20 journals was investigated and their frequency was related to factors like the year of publication, and the citation and publication year of the initial study. 

# Overview analysis: how often do articles mention the term replicat*?
The key dependent variable of the first part of this study was the rate of replication mention for journals relevant to the field of experimental linguistics. 
We intended to answer the following research questions: How many replication studies have been published in journals representative for experimental linguistic research? 
How did the rate change over time <!--KK: oops, we don't have any data by years--> and how does it relate to journal policy, impact factor, and publication type?

### Material and methods
<!--provide sufficient details to allow the work to be reproduced by an independent researcher, describe any modifications to existing methods-->

The material and methods have been preregistered at DATA at the Open Science Framework and can be inspected [here: https://osf.io/9ceas/](https://osf.io/9ceas/). 

In order to determine the rates of replication mention for individual journals, we drew on a method introduced by @makel_replications_2012. 
First, a sample of 100 journals relevant to the field of experimental linguistics has been identified by making use of the search engine ["Web of Science" (https://webofknowledge.com)](https://webofknowledge.com). We restricted the search results to journals in the web of science category "Linguistics" which had at least 100 articles published and a high ratio of articles containing the term "experiment\*" in title, abstract or keywords <!--TR: We might need to be more specific and add the full procedure either here or in an appendix or refer to prereg link-->. All English language articles from the full available range of complete years (1945-2020) were taken into account. We selected the top 100 journals according to their ratio of experimental studies. The full list of journals can be inspected [here: https://osf.io/q2e9k/](https://osf.io/q2e9k/). <!--KK: include in appendix?-->
The procedure described above helped us to identify journals relevant for the field of experimental linguistics. 
As a second step, the total number of articles containing the search term "replicat\*" in title, abstract or keywords was obtained via Web of Science search for the 100 sampled journals. 
Following the method used by Makel et al. [-@makel_replications_2012] the rates of replication mention are calculated by dividing the number of articles containing the term "replicat\*" by the total number of articles for each journal. As we were only interested in experimental linguistic studies, we only included articles containing the search term "experiment\*" in this formula.

In order to relate the rate of replication mention to journal policies, we further examined the journals' submission guidelines adopting a procedure used by Martin and Clarke [-@martinclarke_policies_2017]. 
They grouped psychology journals into four classes determined by what was stated in the "instructions to authors" and "aims and scope" sections on the websites of the respective journals:
(1) Journals which stated that they accepted replications;
(2) Journals which did not state they accepted replications but did not discourage replications either;
(3) Journals which implicitly discouraged replications through the use of emphasis on the scientific originality of submissions,
(4) Journals which actively discouraged replications by stating explicitly that they did not accept replications for publication [@martinclarke_policies_2017, p. 3].
For our analysis, we only distinguished between those journals explicitly encouraging replication studies (1) and those that do not (2-4). 

Journal impact factors were extracted via Journal Citation Reports (https://jcr.clarivate.com). 
The 2019 journal impact factors are calculated by dividing the citations in 2019 to items published in 2017 and 2018 by the total number of citable items in 2017 and 2018. <!--TR: explanation of JIF can go in foot note-->

Furthermore, we assessed via Web of Science whether journals published open access. 
We distinguished between three categories: journals which are listed in the Directory of Open Access Journals (DOAJ) ("DOAJ gold"), journals with some articles being published as open access articles ("partial") and journals with no option to publish open access ("no").

### Results
<!--clear and concise, include tables and figures-->

```{r number_articles, echo=FALSE}
sum_articles = sum(df$no_ling) # 51272
sum_exp = sum(df$no_exp) # 8006
sum_repl = sum(df$no_replic) # 347
mention_rate = round((sum_repl / sum_exp), 3)#

```

Out of the `r sum_articles` articles in our sample, `r sum_exp` mentioned the term experiment* in title, abstract, or keywords. 
Out of these articles, `r sum_repl` contained the term replic* in either abstract or title. 
Thus the rate of mentioning the term replic* is `r mention_rate`. 
<!--TR: Compare to Makel and others and relate to the fact that we already filtered non-experimental out-->

```{r mean and variance, echo=FALSE}
mean_rate = round(mean(df$replic_rate), 3) # ~ 2.7%
median_rate = round(median(df$replic_rate), 3) # ~ 1.6%

min_rate = round(min(df$replic_rate), 3) #  0%
max_rate = round(max(df$replic_rate), 3) # ~ 12.8%
std_rate = round(sd(df$replic_rate), 3) # ~ 3.3%
```

```{r rep rates of 0, echo=FALSE}
zero_mention = nrow(df[df$replic_rate == 0,])
# 43 of 100 journals have rates of replication mention = 0
encouraged = nrow(df[df$binary_policy == 1,])

```

The distribution of the mention rate substantially varied across journals ranging from `r min_rate` to `r max_rate`. 
`r zero_mention` journals did not mention the term in any of their articles.
The median mention rate across journals is `r median_rate` (SD = `r std_rate`). 

Following preregistered protocol, we estimated the mention rate as predicted relative to the following factors: 
journal impact factors (continuous), open access (binary: open access journal or not), and replication policies (binary: either explicitly encourage or not). 
We used Bayesian parameter estimation based on generalized linear regression models with a binomial link function. 
The model will be fitted to the proportion of replication mentions per journal using the R package brms [@burkner_brms_2016]. 
We used weakly informative normal priors centered on 0 (sd = 2.5) for the intercept and Cauchy priors centered on zero (scale = 2.5) for all population-level regression coefficients. 
These priors are what is referred to as regularizing [@gelman_weakly_2008], i.e. our prior assumption is agnostic as to whether the predictors affect the dependent variable, thus making our model conservative with regards to the predictors under investigation. 
Four sampling chains with 2000 iterations each will be run for each model, with a warm-up period of 1000 iterations.
For relevant predictor levels and contrasts between predictor levels, we will report the posterior probability for the rate of replication mention. 
We summarize these distributions by reporting the posterior mean and the 95% credible intervals (calculated as the highest posterior density interval). 


```{r mention_model_prep, echo=FALSE}

# extract coefficients
mention_intercept_est <- round(plogis(fixef(mention_model)[1,1]),2)
mention_intercept_lb <- round(plogis(fixef(mention_model)[1,3]),2)
mention_intercept_hb <- round(plogis(fixef(mention_model)[1,4]),2)

mention_jif_est <- round(fixef(mention_model)[2,1],2)
mention_jif_lb <- round(fixef(mention_model)[2,3],2)
mention_jif_hb <- round(fixef(mention_model)[2,4],2)

## extract predicted values for JIF
predicted_values <- mention_model %>%
  spread_draws(b_Intercept, b_jif) %>%
  ### make a list of relevant value range of logRT
  mutate(jif = list(seq(0, 4, 0.1))) %>% 
  unnest(jif) %>%
  ### transform into proportion space using the plogis function
  mutate(pred = plogis(b_Intercept + b_jif*jif)) %>%
  pivot_longer(cols = pred,
               names_to = "factors") %>% 
  group_by(jif) %>%
  summarise(pred_m = mean(value, na.rm = TRUE),
            pred_low = quantile(value, prob = 0.025),
            pred_high = quantile(value, prob = 0.975)) 

```

The model estimates a replication mention rate of `r mention_intercept_est` [`r mention_intercept_lb`, `r mention_intercept_hb`] at a JIF of 0 and estimates a rather robust increase of the rate with each unit of JIF (log odds = `r mention_jif_est` [`r mention_jif_lb`, `r mention_jif_hb`]). FigureX illustrates this relationship.

<!-- FIGURE -->
(ref:plot_mention_jif) Rate of mentioning the term 'replic*' across sampled journals plotted against journal impact factor. Each point represents one journal. Point size indicates the ratio of papers categorized as experimental (i.e. larger points indicate journals with more experimental articles. Line and shading indicate model predictions and 95% credible intervals.

```{r plot_mention_jif, fig.cap = "(ref:plot_mention_jif)", out.width="100%", fig.align = 'center'}

## plot predicted values against data
mention_jif <- 
ggplot(data = predicted_values, 
       aes(x = jif, 
           y = pred_m)) +
  geom_ribbon(aes(ymin = pred_low, 
                  ymax = pred_high), 
              alpha = 0.4,
              fill = "grey") +
  geom_line(color = "black", 
            size = 1.5) +
  geom_point(data = df, 
             aes(x = jif, 
                 y = replic_rate,
                 size = exp_ratio),
             pch = 21,
             alpha = 0.7, 
             color = "white", 
             fill = col_purple) +
  scale_fill_manual(values = col_purple) +
  ylab("Predicted rate of replication mention") +
  ylim(0, 0.20) +
  xlim(0, 4) +
  labs(title = "Rate of mentioning the term 'replicat*'",
       subtitle = "   ",
       y = "Predicted rate of replication mention\n",
       x = "\nJournal Impact Factor") +
  theme_minimal() +
  theme(legend.position = "none",
              legend.key.height = unit(2,"line"),
              legend.title = element_text(face = "bold", size = 12),
              legend.text = element_text(size = 12),
              strip.background = element_blank(),
              strip.text = element_text(size = 12, face = "bold"),
              panel.spacing = unit(2, "lines"),
              panel.border = element_blank(),
              plot.background = element_rect(fill = "transparent", colour = NA),
              strip.text.y = element_text(size = 12, hjust = 0),
              axis.text = element_text(size = 12),
              axis.line = element_blank(),
              axis.title = element_text(size = 12, face = "bold"),
              plot.title = element_text(size = 14, face = "bold"),
              plot.margin = unit(c(0.6,0.6,0.6,0.6),"cm"))

mention_jif

```

<!-- END: FIGURE -->

Given the small number of journals that explicitly encourage direct replications (`r encouraged` out of 98), 


```{r rate of replication mention, echo=FALSE}

mention_by_exp_ratio <- df %>% 
  select(journals, exp_ratio, replic_rate) %>% 
  arrange(desc(exp_ratio)) %>% 
  mutate(
    Journals = factor(journals, ordered=T),
    "Ratio of experimental studies" = exp_ratio,
    "Rate of replication mention" = round(replic_rate*100, 2)
  ) %>% 
  select(Journals, "Ratio of experimental studies", "Rate of replication mention")
head(mention_by_exp_ratio, 10)


mention_by_rate <- df %>% 
  select(journals, replic_rate) %>% 
  arrange(desc(replic_rate)) %>% 
  mutate(
    Journals = journals,
    "Rate of replication mention" = round(replic_rate*100, 2) 
  ) %>% 
  select(Journals, "Rate of replication mention")
head(mention_by_rate, 1)
```





```{r, echo=FALSE}
head(guidelines)
```

```{r policy, echo=FALSE}
no_encourage <- df %>% 
  filter(binary_policy == 1) %>% 
  count() 
# --> only 2 out of 100 journals explicitly encourage submission of replication studies
```

### Discussion
- too little replication attempts in experimental linguistics
- journals guidelines generally don't encourage replication studies
- ...

# Detailed analysis: types and contributing factors
The second part of the study aimed at obtaining a better understanding of the underlying mechanisms of replication attempts published in the field of experimental linguistics. 
Because the term "replication" is commonly used in ambiguous ways, the articles that contained the search term "replicat\*" required further analysis to determine whether the articles in question indeed reported a replication study or used the term in a different way. 

We were interested in which kinds of replication studies are published and which factors contribute to their publication. 
We aimed at investigating what types of replication studies are prevalent in the field. 
We were further interested in the relationship of direct replications and whether the paper was published as open access or not, the number of citations of the initial study and the years between publication of the initial study and the replication attempt.

### Material and methods
<!--provide sufficient details to allow the work to be reproduced by an independent researcher, describe any modifications to existing methods -->
The material and methods have been preregistered on Open Science Framework and can be inspected [here: https://osf.io/9ceas/](https://osf.io/9ceas/). 

From the superset of 100 journals obtained above, the first 20 journals (i.e. those journals with the highest proportion of experimental studies) were selected for a more detailed analysis while excluding journals for which less than 2 hits (TS=(replicat\*)) could be obtained (see [here](https://osf.io/f3yp8/) for a list of article counts per journal: https://osf.io/f3yp8/). 
Because of the skewed distribution of our sample (114 hits for Journal of Memory and Language, and less than 40 for all other journals), we randomly selected (see [here](https://osf.io/6vfpe/) for details) 50 out of the 114 articles for the Journal of Memory and Language to achieve a more balanced distribution of papers across journals.
The sampling procedure above resulted in 210 possible self-labeled replication studies. 

In a first step, we identified whether the article in question indeed presented a replication study or not. The relevant parts of the papers were title and abstract of the paper, sentences around occurrences of the search term "replicat" as well as the paragraph before the Methods section and the first paragraph of the Discussion section (following the procedure specified by Makel et al. [-@makel_replications_2016]).
If the authors explicitly claimed that (one of) their research aim(s) was to replicate or reproduce findings or methods of an initial study, this article was treated as a replication. 
It then qualified for further analysis after the coding scheme that can be viewed [here](https://osf.io/ct2xj/): https://osf.io/ct2xj/. 

When extracting number and types of changes made to the initial study, we assumed that the authors of a replication study did not make any drastic changes *without* reporting them. 
The replication studies were classified according to three types: direct replication (0 changes), partial replication (1 change) and conceptual replication (2 or more changes), following Marsden et al. [-@marsden_replication_2018]. 
We noted the nature of the change as one of the following categories (yes/no): experimental paradigm, sample, materials/experimental set-up, dependent variable, independent variable, and control. We also noted the language under investigation.
The information on whether the article was published open access as well as citation counts and years of publication for both studies were obtained from Web of Science.
An author overlap was attested when one of the authors was a (co-)author on both articles.

### Results
<!--clear and concise, include tables and figures-->

```{r loading data, echo=FALSE}
# loading the data
coded_articles = read.csv("../../data/Coding_Articles.csv", sep="|")
nr_replications = coded_articles %>% filter(replication == "1") %>% count() # 117 are replications
```
```{r success, echo=FALSE}
# How many of the direct replications were (self-labeled) successful?
direct_reps = coded_articles %>% filter(type_replication == "direct") %>% count() # 8 direct replications
success_reps = coded_articles %>% filter(type_replication == "direct") %>% filter(success == "1") %>% count() # 7 successful
success_rate = (success_reps / direct_reps) * 100
# --> 87,5%
```
```{r years between, echo=FALSE}
# What is the mean and median number of years between an initial and a replication study?
coded_articles %>% 
  filter(cit_init_study != "same") %>% # exclude inner-paper replications
  summarise(mean_years = mean(years_between, na.rm = TRUE)) # ~ 8,81 years
coded_articles %>% 
  filter(cit_init_study != "same") %>% # exclude inner-paper replications
  summarise(median_years = median(years_between, na.rm = TRUE)) # 7 years
```
```{r citation, echo=FALSE}
# What is the mean and median citation count of initial studies before a replication study was published?
coded_articles %>% 
  filter(cit_init_study != "same") %>% # exclude inner-paper replications
  summarise(mean_cit_init = mean(init_cit_til_rep, na.rm = TRUE)) # ~ 41,1 citations
coded_articles %>% 
  filter(cit_init_study != "same") %>% # exclude inner-paper replications
  summarise(median_cit_init = median(init_cit_til_rep, na.rm = TRUE)) # 19,5 citations
```
```{r plot cit and years direct, echo=FALSE, message=FALSE}
# preparing data for plot

# wrangling citation data
cit_data <- coded_articles %>% 
  filter(type_replication == "direct") %>% # only include direct replications
  mutate(
    obs_nr = c(1:8)
  ) %>% 
  select(obs_nr, rep_citation, init_cit_til_rep) %>% 
  pivot_longer(cols = c(rep_citation, init_cit_til_rep), values_to = "citation") %>% 
  mutate(
    study_kind = ifelse(name == "rep_citation", "replication", "initial")
  ) %>% 
  select(obs_nr, study_kind, citation)

# wrangling year of publication data
pub_data <- coded_articles %>% 
  filter(type_replication == "direct") %>% # only include direct replications
  mutate(
    obs_nr = c(1:8)
  ) %>%
  select(obs_nr, pub_year, year_init_study) %>% 
  pivot_longer(cols = c(pub_year, year_init_study), values_to = "years") %>% 
  mutate(
    study_kind = ifelse(name == "pub_year", "replication", "initial")
  ) %>% 
  select(obs_nr, study_kind, years)

# creating one data frame for plotting
plot_data <- full_join(cit_data, pub_data)

# creating a plot
plot_data %>% 
  ggplot(aes(x = years, y = citation, color = study_kind, group = obs_nr)) +
  geom_point() +
  geom_line(color = "lightgrey") +
  labs(
    x = "years",
    y = "citations",
    title = "After how many years and how many citations do replication studies get published?"
  )
```
```{r plot cit and years all, echo=FALSE, message=FALSE, warning=FALSE}
# preparing data for plot

# wrangling citation data
cit_data <- coded_articles %>% 
  filter(replication == "1") %>% # only include replications
  mutate(
    obs_nr = c(1:117)
  ) %>% 
  select(obs_nr, rep_citation, init_cit_til_rep) %>% 
  pivot_longer(cols = c(rep_citation, init_cit_til_rep), values_to = "citation") %>% 
  mutate(
    study_kind = ifelse(name == "rep_citation", "replication", "initial")
  ) %>% 
  select(obs_nr, study_kind, citation)

# wrangling year of publication data
pub_data <- coded_articles %>% 
  filter(replication == "1") %>% # only include replications
  mutate(
    obs_nr = c(1:117)
  ) %>%
  select(obs_nr, pub_year, year_init_study) %>% 
  pivot_longer(cols = c(pub_year, year_init_study), values_to = "years") %>% 
  mutate(
    study_kind = ifelse(name == "pub_year", "replication", "initial")
  ) %>% 
  select(obs_nr, study_kind, years)

# creating one data frame for plotting
plot_data <- full_join(cit_data, pub_data)

# creating a plot
plot_data %>% 
  ggplot(aes(x = years, y = citation, color = study_kind, group = obs_nr)) +
  geom_point() +
  geom_line(color = "lightgrey") +
  labs(
    x = "years",
    y = "citations",
    title = "After how many years and how many citations do replication studies get published?"
  )
```
```{r inner paper, echo=FALSE}
# How many replication studies were inner-paper replications?
reps_inner_paper = coded_articles %>% 
  filter(replication == "1") %>% 
  filter(cit_init_study == "same") %>% 
  count() 
# --> 37
inner_paper = (reps_inner_paper / nr_replications) * 100
# --> ~ 31,62 %
```
```{r language rep, echo=FALSE}
# How many replication studies were replications in a different language?
reps_diff_lang = coded_articles %>% 
  filter(replication == "1") %>% 
  filter(str_detect(comments, "replication in a different language")) %>% 
  count()
# --> 18
diff_lang = (reps_diff_lang / nr_replications) * 100
# --> ~ 15,38 %
```
```{r same journal, echo=FALSE}
# How many replication studies were published in the same journal?
journal_init = factor(coded_articles$journal_init_study, levels = levels(coded_articles$journal)) # set levels equal
reps_same_journal = coded_articles %>% 
  filter(journal == journal_init) %>% 
  filter(cit_init_study != "same") %>% # exclude inner-paper studies
  count()
# --> 16 (52 without exclusion)
same_journal = (reps_same_journal / nr_replications) * 100
# --> ~ 13,68 %
```
```{r jml, echo=FALSE}
# having a closer look at jml
jml_data <- coded_articles %>% 
  filter(journal == "JOURNAL OF MEMORY AND LANGUAGE")

## how many really experimental
jml_exp <- jml_data %>% 
  filter(experimental == "1") 
jml_exp_ratio = count(jml_exp)/count(jml_data) 
# ---> 50 = 100%

## of those how many actual replications
jml_reps <- jml_exp %>% 
  filter(replication == "1")
jml_rep_ratio = count(jml_reps)/count(jml_exp)
# ---> 34 = 68%

## of those what types of replications
round(xtabs(~type_replication, jml_reps) / nrow(jml_reps), 2)
### 0.44 conceptual
### 0.09 direct --> compared to 7% over all journals
### 0.47 partial

## of those author overlap
round(xtabs(~type_replication + auth_overlap, jml_reps) / nrow(jml_reps), 2)
###            no   yes 
### conceptual 0.18 0.26
### direct     0.06 0.03
### partial    0.24 0.24

# all journals:
###            no   yes 
### conceptual 0.26 0.30
### direct     0.03 0.04
### partial    0.15 0.21

## overall direct independent replication rate 
round(nrow(jml_data[jml_data$replication == 1 &
                jml_data$type_replication == "direct" &
                jml_data$auth_overlap == 0,]) / nrow(jml_data), 3)
### 0.04 = 4% compared to 0.015 = 1,5%
```

### Discussion

# General discussion
- compare rate of replication mention to previous studies in different fields --> broader picture

### Caveats
This procedure is necessarily only a rough proxy of relevant experimental linguistic articles published in the field and several articles might thus have been overlooked and not been included in the analysis.


# Appendices
identified as A, B, etc.


References {#references .unnumbered}
==========
