---
title: Assessing replication rates in journals of experimental linguistics
author:
  - name: Kristina Kobrock
    email: kkobrock@uni-osnabrueck.de
    affiliation: University of Osnabrück
    footnote: 1
  - name: Timo B. Roettger
    #email: bob@example.com
    affiliation: Universitetet i Oslo
address:
  - code: University of Osnabrück
    address: Institute of Cognitive Science, Wachsbleiche 27, 49090 Osnabrück
  - code: Universitetet i Oslo
    address: Department of Linguistics and Scandinavian Studies
footnote:
  - code: 1
    text: "Corresponding Author"
abstract: |
  This is the abstract. ~150 words, avoid references, optional graphical abstract, keywords (max. 6, avoid abbreviations, AE spelling)

  It consists of two paragraphs. 

journal: "Journal of Memory and Language"
date: "`r Sys.Date()`"
bibliography: r-references.bib
#linenumbers: true
#numbersections: true
csl: elsevier-harvard.csl
output: rticles::elsevier_article

---

```{r setup, include=FALSE, warning=FALSE, echo=FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      cache = TRUE, 
                      warning = FALSE,
                      message = FALSE)

# nifty code using the pacman package
# it checks if the packages specified below are installed, if not, they will be installed, if yes, they will be loaded
if (!require("pacman")) install.packages("pacman")
pacman::p_load(rstudioapi, truncnorm, tidyverse, brms, ggdist, distributional, 
               ggstream, tidybayes, bayesplot, rticles, knitr, ggalluvial)

# set the current working directory to the one where this file is
 #current_working_dir <- dirname(rstudioapi::getActiveDocumentContext()$path) KK: not working when knitting
 current_working_dir <- getwd()
 setwd(current_working_dir)

## global color scheme / non-optimized
### purple
col_purple = "#7b3294"
### light purple
col_lightPurple = "#c2a5cf"
### green
col_green = "#008837"
### light green
col_lightGreen = "#a6dba0"

# loading the data
mention <-  read.csv("../../data/mention.csv")
guidelines  <-  read.csv("../../data/guidelines.csv")
coded_articles  <-  read.csv("../../data/Coding_Articles.csv", sep="|")  

# load models
mention_model <- readRDS(file = "../../data/repl_mention_mdl.RDS")

# merge dfs
df <- full_join(mention, guidelines)

# delete older journals that have been renamed
df <- df %>% 
  filter(!(journals %in% c("LANGUAGE AND COGNITIVE PROCESSES", "LITERARY AND LINGUISTIC COMPUTING")))

# recode jif
df$jif <- as.numeric(ifelse(df$jif == "not retrievable", NA, df$jif))

# NAs would be dropped from model. What to do? Either assume 0 or separate models

# recode open access as binary
df$openaccess <- ifelse(df$openaccess == "DOAJ gold", 1, 0)

```

# Introduction
<!--state the objectives of the work and provide an adequate background, avoiding a detailed literature survey or a summary of the results-->

The replication of results is an integral part of cumulative experimental science [e.g., @campbell_reforms_1969; @rosenthal_replication_1990; @zwaan_making_2018]. 
Yet, various scientific disciplines are currently facing what is often referred to as the "replication crisis", a state of affairs that is characterized by an alarming amount of failed replication attempts [@fidler_reproducibility_2018]. <!--KK: I know you said for the preregistration that we should refrain from these strong words so that we don't scare potential readers and publishers off - but on the other hand these are also buzz words that could lead to our study being found and read, aren't they?-->
Here, we define replications as studies that arrive at the same scientific conclusions as an initial study, collecting new data and completing new analyses by using the same methodology [see @barba_terminologies_2018 for a comprehensive overview of different terminological uses].
Coordinated efforts to replicate published findings have uncovered surprisingly low rates of successful replications in fields such as psychology [@open_science_collaboration_estimating_2015], economics [@camerer_evaluating_2016] and social sciences [@camerer_evaluating_2018]. Researchers from these fields report low replication rates ranging from 0.13\% in the education sciences to 1.07\% in psychology publications. <!--KK: footnote? measure: whether the effect size of the replication study fell wihtin the 95% confidence interval of the original-->

One driving factor of this lack of replicability is an asymmetric incentive system that rewards novel confirmatory findings over direct replications and null results. Replication studies are not very popular because the necessary time and resource investment are not appropriately rewarded in contemporary academic incentive systems [e.g., @koole_rewarding_2012; @nosek_scientific_2012]. Both successful replications [@madden_how_1995] and repeated failures to replicate [e.g., @doyen_behavioral_2012] are rarely published, and even if they are, they usually appear in less prestigious outlets than the original findings. 
These dynamics lead to an abundance of positive findings in the absence of possible conflicting negative evidence [see also e.g. @fanelli_pressures_2010].

Experimental linguistics shares research practices that have been shown to decrease the replicability of findings. 
Thus, there are raising concerns about a similarly low number of replication studies conducted and published in this field [e.g. @marsden_replication_2018; @roettger_toward_2019].
A number of failed replication attempts reported in various subfields of linguistics indicate that these concerns are warranted [e.g. in language comprehension: @papesh_just_2015; predictive processing: @nieuwland_large-scale_2018; among others: @chen_chinese_2007; @stack_failure_2018; @westbury_implicit_2018].
In order to thoroughly understand and be able to address this problem, it is important to assess the number of replication attempts in the field of experimental linguistics and their contributing factors.

<!--TR: need to discuss the Marsden et al paper here -->
One subfield of experimental linguistics, second language research, has already been systematically reviewed. @marsden_replication_2018 coded 67 self-labeled L2 replication studies for 136 characteristics. Their results indicate that for every 400 articles, only one replication study is published translating to a replication rate of about 0.25\%. Their sample did not include any direct replication attempts. Changes made to the initial study were very diverse and numerous. These results motivate to assess replication rates and contributing factors for the more general field of experimental linguistics.

The present study assessed the frequency and typology of replication studies that have been published in a representative sample of experimental linguistic journals from 1988 to 2020. 
Our study aimed at answering two main questions: 
How many direct replications are published in experimental linguistics?
Are there factors that affect the replication rates that are either found at the journal level (e.g. journal policies, open access, journal impact factor, etc.) or at the study level (e.g. composition of authors, investigated language, etc.)?
The study consisted of two analyses: 
First, we assessed the rate of mentioning the term replication (search string: replicat\*) across 100 linguistic journals. 
Second, we categorized the type of replication studies (direct, partial, conceptual) in a subset of twenty journals. We then related their prevalence to factors like the years of publication, and the citations of both initial and replication study. 

# How often do journals mention the term replication?
The key dependent variable of the first part of this study was the rate of replication mention for journals relevant to the field of experimental linguistics. 

### Material and methods
<!--provide sufficient details to allow the work to be reproduced by an independent researcher, describe any modifications to existing methods-->
The study design has been preregistered at 2021-03-08 and can be inspected [here: https://osf.io/9ceas/](https://osf.io/9ceas/). 

In order to determine the rates of replication mention for individual journals, we drew on a method introduced by @makel_replications_2012. 
First, a sample of 100 journals relevant to the field of experimental linguistics has been identified by making use of the search engine ["Web of Science" (https://webofknowledge.com)](https://webofknowledge.com) at 2021-03-03. We restricted the search results to journals in the web of science category "Linguistics" which had at least 100 articles published and a high ratio of articles containing the term "experiment\*" in title, abstract or keywords <!--TR: We might need to be more specific and add the full procedure either here or in an appendix or refer to prereg link-->. All articles categorized as written in English between 1945-2020 were taken into account. We selected a subset of those 100 journals that had the highest ratio of mentioning the term "experiment\*". 
```{r exp_ratio}
exp_ratio_median = round(median(df$exp_ratio),1) # 11.5
exp_ratio_min = round(min(df$exp_ratio),1) # 6.1
exp_ratio_max = round(max(df$exp_ratio),1) # 60.3

```
The ratio between overall number of articles and those articles mentioning the term "experiment\*" ranged between `r exp_ratio_min` and `r exp_ratio_max` (with a median of `r exp_ratio_median`). Two journals, namely "Language and Cognitive Processes" and "Language, Cognition and Neuroscience" had to be excluded because it turned out during analysis that both journals have been renamed in 2013 and that they have already been included in our sample under the new name.


The full sample of journals can be inspected [here: https://osf.io/q2e9k/](https://osf.io/q2e9k/) or in the appendix of this article. <!--KK: include in appendix?--> <!--TR: Yes one pretty table!-->
Albeit a rough proxy, this procedure helped us identifying journals relevant to the field of experimental linguistics.

After journal selection, we obtained the total number of articles containing the search term "replicat\*" in title, abstract or keywords via Web of Science search. 
Following the method used by Makel et al. [-@makel_replications_2012], the rates of replication mention are calculated by dividing the number of articles containing the term "replicat\*" by the total number of articles for each journal. As we were only interested in experimental linguistic studies, we only included articles containing the search term "experiment\*" in this formula.

Replication mentions are related to three journal properties: Journal policies related to replication studies, journal impact factor and whether the journal is open access or not.
We examined the journals' submission guidelines adopting a method suggested by Martin and Clarke [-@martin_are_2017]. 
They grouped psychology journals into four categories dependent on whether they (explicitly) encourage replication studies or not in their "instructions to authors" and "aims and scope" sections on the journal websites. For our analysis, we only distinguished between those journals explicitly encouraging replication studies and those that do not. 

We extracted Journal Impact Factors via Journal Citation Reports (https://jcr.clarivate.com).^[The 2019 journal impact factors are calculated by dividing the citations in 2019 to items published in 2017 and 2018 by the total number of citable items in 2017 and 2018.]

Whether journals offered an open access publication or not was assessed via Web of Science. 
We distinguished between three categories: journals which are listed in the Directory of Open Access Journals (DOAJ) ("DOAJ gold"), journals with some articles being published as open access articles ("partial") and journals with no option to publish open access ("no"). We grouped journals with partial or no option to publish open access together for analysis resulting in a binary categorization of open access.

### Results
<!--clear and concise, include tables and figures-->

```{r number_articles}
sum_articles = sum(df$no_ling) # 51272
sum_exp = sum(df$no_exp) # 8006
sum_repl = sum(df$no_replic) # 347
mention_rate = round((sum_repl / sum_exp), 3)#

```

Out of the `r sum_articles` articles in our sample, `r sum_exp` mentioned the term 'experiment\*' in title, abstract, or keywords. 
Out of these articles, `r sum_repl` contained the term "replicat\*". 
Thus `r round(mention_rate * 100, 2)`\% of all experimental articles in our sample mentioned the term "replicat\*". 

```{r mean and variance, echo=FALSE}
mean_rate = round(mean(df$replic_rate)*100 , 1) # ~ 2.7%
median_rate = round(median(df$replic_rate) *100, 1) # ~ 1.6%

min_rate = round(min(df$replic_rate) *100, 1) #  0%
max_rate = round(max(df$replic_rate) *100, 1) # ~ 12.8%
std_rate = round(sd(df$replic_rate) *100, 1) # ~ 3.3%
```

```{r rep rates of 0, echo=FALSE}
zero_mention = nrow(df[df$replic_rate == 0,])
nonzero_mention = nrow(df[df$replic_rate != 0,])
# 43 of 100 journals have rates of replication mention = 0
encouraged = nrow(df[df$binary_policy == 1,])
openaccess = nrow(df[df$openaccess == 1,])
```

The distribution of the rate of replication mention substantially varies across journals ranging from `r min_rate` to `r round(max_rate * 100, 2)`\%. 
Overall, almost half of all journals (n = `r zero_mention`) did not mention the term in any of their articles.
The median mention rate across journals is `r median_rate` (SD = `r std_rate`). 
This rate is comparable to the 1.6\% that @makel_replications_2012 have reported on in their study on psychology. <!--KK: Shouldn't that be only mentioned in the discussion as it's already (kind of) interpreting?-->
Figure X illustrates the variation across those journals that exhibited at least one mention of the term.

```{r topten_plot,  out.width="100%", fig.align = 'center', fig.width = 5, fig.asp = 1.6}

# should make a list of shortened journal names, shortened strings for now

topten <- 
  df %>% arrange(desc(replic_rate)) %>%
  select(journals, exp_ratio, replic_rate) %>% 
  top_n(nonzero_mention, replic_rate) %>% 
  mutate(journals = fct_reorder(journals, replic_rate)) %>%
  ggplot(aes(x = replic_rate * 100, 
             y = journals,
             color = replic_rate * 100)) +
  geom_segment(aes(x = 0, 
                   xend = replic_rate * 100,
                   y = journals,
                   yend = journals)) +
  geom_point(size = 3) + 
  scale_color_gradient(low = col_purple,
                       high = col_green)+
 # geom_segment()
  scale_y_discrete(label = function(x) stringr::str_trunc(x, 20))+
  xlim(0, 15) +
  labs(y = " ",
       x = "\nProportion of mentions in %\n") +
  theme_minimal() +
  theme(legend.position = "none",
              legend.key.height = unit(2,"line"),
              legend.title = element_text(face = "bold", size = 12),
              legend.text = element_text(size = 12),
              strip.background = element_blank(),
              strip.text = element_text(size = 12, face = "bold"),
              panel.spacing = unit(2, "lines"),
              panel.border = element_blank(),
              plot.background = element_rect(fill = "transparent", colour = NA),
              strip.text.y = element_text(size = 12, hjust = 0),
              axis.text.x = element_text(size = 12),
              axis.text.y = element_text(size = 8),
              axis.line = element_blank(),
              axis.title = element_text(size = 12, face = "bold"),
              plot.title = element_text(size = 14, face = "bold"),
              plot.margin = unit(c(0.6,0.6,0.6,0.6),"cm"))

topten

```

Following preregistered protocol, we statistically estimated the mention rate as predicted relative to the following factors: 
journal impact factors (continuous), open access (binary: open access journal or not), and replication policies (binary: either explicitly encourage or not). 
We used Bayesian parameter estimation based on generalized linear regression models with a binomial link function. 
The model was fitted to the proportion of replication mentions per journal using the R package brms [@burkner_brms_2016]. 
We used weakly informative normal priors centered on 0 (sd = 2.5) for the intercept and Cauchy priors centered on zero (scale = 2.5) for all population-level regression coefficients. 
These priors are what is referred to as regularizing [@gelman_weakly_2008], i.e. our prior assumption is agnostic as to whether the predictors affect the dependent variable, thus making our model conservative with regards to the predictors under investigation. 
Four sampling chains with 2000 iterations each have been run for each model, with a warm-up period of 1000 iterations.
For relevant predictor levels and contrasts between predictor levels, we report the posterior probability for the rate of replication mention. 
We summarize these distributions by reporting the posterior mean and the 95\% credible intervals (calculated as the highest posterior density interval). 


```{r mention_model_prep, echo=FALSE}

# extract coefficients
mention_intercept_est <- round(plogis(fixef(mention_model)[1,1]) * 100, 1)
mention_intercept_lb <- round(plogis(fixef(mention_model)[1,3]) * 100, 1)
mention_intercept_hb <- round(plogis(fixef(mention_model)[1,4]) * 100, 1)

mention_jif_est <- round(fixef(mention_model)[2,1],2)
mention_jif_lb <- round(fixef(mention_model)[2,3],2)
mention_jif_hb <- round(fixef(mention_model)[2,4],2)

mention_oa_est <- round(fixef(mention_model)[3,1],2)
mention_oa_lb <- round(fixef(mention_model)[3,3],2)
mention_oa_hb <- round(fixef(mention_model)[3,4],2)

mention_poli_est <- round(fixef(mention_model)[4,1],2)
mention_poli_lb <- round(fixef(mention_model)[4,3],2)
mention_poli_hb <- round(fixef(mention_model)[4,4],2)

## extract predicted values for JIF
predicted_values <- mention_model %>%
  spread_draws(b_Intercept, b_jif) %>%
  ### make a list of relevant value range of logRT
  mutate(jif = list(seq(0, 4, 0.1))) %>% 
  unnest(jif) %>%
  ### transform into proportion space using the plogis function
  mutate(pred = plogis(b_Intercept + b_jif*jif)) %>%
  pivot_longer(cols = pred,
               names_to = "factors") %>% 
  group_by(jif) %>%
  summarise(pred_m = mean(value, na.rm = TRUE),
            pred_low = quantile(value, prob = 0.025),
            pred_high = quantile(value, prob = 0.975)) 

```

The model estimates the proportion of replication mentions as `r mention_intercept_est`\% [`r mention_intercept_lb`, `r mention_intercept_hb`] at a journal impact factor (JIF) of 0 and estimates an increase of the proportion with each integer unit of JIF (log odds = `r mention_jif_est` [`r mention_jif_lb`, `r mention_jif_hb`]). Figure X illustrates this relationship. 

```{r correlation_jif_exp_ratio, echo=FALSE}
cor_jif_exp <- round(cor(df$exp_ratio, df$jif, use = "complete.obs", method = c("spearman")),2)
```

However, further explorations indicate that JIF is highly correlated with the number of experimental studies reported in a journal (Spearman correlation = `r cor_jif_exp`). 
This correlation makes intuitively sense. 
Experimental linguistic articles are often associated with psychology adjacent fields, possibly attracting a broader audience and being cited more widely. 
Given this correlation, it remains unclear if the term "replicat\*" is used more often in high impact journals or simply more common in journals that generally publish more experimental studies. 

<!--TR: fig.cap = "Rate of mentioning the term 'replic*' across sampled journals plotted against their journal impact factor. Each point represents one journal. Point size indicates the proportion of papers categorized as experimental (i.e. larger points indicate journals with more experimental articles). Line and shading indicate model predictions and 95% credible intervals."-->

```{r plot_mention_jif, out.width="100%", fig.align = 'center'}

## plot predicted values against data
mention_jif <- 
ggplot(data = predicted_values, 
       aes(x = jif, 
           y = pred_m)) +
  geom_ribbon(aes(ymin = pred_low, 
                  ymax = pred_high), 
              alpha = 0.4,
              fill = "grey") +
  geom_line(color = "black", 
            size = 1.5) +
  geom_point(data = df, 
             aes(x = jif, 
                 y = replic_rate,
                 size = exp_ratio,
                 fill = replic_rate),
             pch = 21,
             alpha = 0.7, 
             color = "white") +
  scale_fill_gradient(low = col_purple,
                       high = col_green) +
  ylab("Predicted rate of replication mention") +
  ylim(0, 0.20) +
  xlim(0, 4) +
  labs(title = "Rate of mentioning the term 'replicat*'",
       subtitle = "   ",
       y = "Predicted rate of replication mention\n",
       x = "\nJournal Impact Factor") +
  theme_minimal() +
  theme(legend.position = "none",
              legend.key.height = unit(2,"line"),
              legend.title = element_text(face = "bold", size = 12),
              legend.text = element_text(size = 12),
              strip.background = element_blank(),
              strip.text = element_text(size = 12, face = "bold"),
              panel.spacing = unit(2, "lines"),
              panel.border = element_blank(),
              plot.background = element_rect(fill = "transparent", colour = NA),
              strip.text.y = element_text(size = 12, hjust = 0),
              axis.text = element_text(size = 12),
              axis.line = element_blank(),
              axis.title = element_text(size = 12, face = "bold"),
              plot.title = element_text(size = 14, face = "bold"),
              plot.margin = unit(c(0.6,0.6,0.6,0.6),"cm"))

mention_jif

```

The model estimates the impact of whether the journal allows for open access publishing or not and whether replications are explicitly encouraged or not both as positive, i.e. the term replication is mentioned more often in both open access journals and  journals that explicitly encourage direct replications. However, the uncertainty around these estimates is substantial (open access: `r mention_oa_est` [`r mention_oa_lb`, `r mention_oa_hb`]; policy: `r mention_poli_est` [`r mention_poli_lb`, `r mention_poli_hb`]) although not surprising given the small number of journals that explicitly encourage direct replications (`r encouraged` out of 98), and the small number of open access journals (`r openaccess` out of 98).


```{r rate of replication mention, echo=FALSE, eval=FALSE}

mention_by_exp_ratio <- df %>% 
  select(journals, exp_ratio, replic_rate) %>% 
  arrange(desc(exp_ratio)) %>% 
  mutate(
    Journals = factor(journals, ordered=T),
    "Ratio of experimental studies" = exp_ratio,
    "Rate of replication mention" = round(replic_rate*100, 2)
  ) %>% 
  select(Journals, "Ratio of experimental studies", "Rate of replication mention")
head(mention_by_exp_ratio, 10)


mention_by_rate <- df %>% 
  select(journals, replic_rate) %>% 
  arrange(desc(replic_rate)) %>% 
  mutate(
    Journals = journals,
    "Rate of replication mention" = round(replic_rate*100, 2) 
  ) %>% 
  select(Journals, "Rate of replication mention")
head(mention_by_rate, 1)
```

```{r, echo=FALSE, eval=FALSE}
head(guidelines)
```

```{r policy, echo=FALSE, eval=FALSE}
no_encourage <- df %>% 
  filter(binary_policy == 1) %>% 
  count() 
# --> only 2 out of 100 journals explicitly encourage submission of replication studies
```


# How many of these mentions are actual replications?
The second part of the study aimed at obtaining a clearer picture about what types of replication studies are published and whether direct replications are becoming more frequent over time.
Because the term "replication" is commonly used in ambiguous ways, the articles that contained the search term "replicat\*" required further analysis to determine whether the articles in question indeed reported a replication study or used the term in a different way. 

### Material and methods
From the superset of 100 journals obtained above, the first 20 journals (i.e. those journals with the highest proportion of experimental studies) were selected for a more detailed analysis while excluding journals for which less than 2 hits (TS=(replicat\*)) could be obtained (see [here](https://osf.io/f3yp8/) for a list of article counts per journal: https://osf.io/f3yp8/). 
Because of the skewed distribution of our sample (114 hits for Journal of Memory and Language, and less than 40 for all other journals), we randomly selected 50 out of the 114 articles for the Journal of Memory and Language in order to achieve a more balanced distribution of papers across journals (see [here](https://osf.io/6vfpe/) for details: https://osf.io/6vfpe/).
The sampling procedure above resulted in 210 possible self-labeled replication studies. 

<!--TR: subset analysis of JML?-->

We identified whether the article in question indeed presented a replication study or not. 
The relevant parts of the papers were title and abstract of the paper, sentences around occurrences of the search term "replicat\*" as well as the paragraph before the Methods section and the first paragraph of the Discussion section (following the procedure specified by Makel et al. [-@makel_replication_2016]).
If the authors explicitly claimed that (one of) their research aim(s) was to replicate the result or methods of an initial study, this article was treated as a replication and was submitted to further analysis according to the preregistered coding scheme which can be inspected [here: https://osf.io/ct2xj/](https://osf.io/ct2xj/). 

When extracting number and types of changes made to the initial study, we assumed that the authors of a replication study did not make any drastic changes *without* reporting them. 
The replication studies were classified according to three types: 
direct replication (0 changes), partial replication (1 change) and conceptual replication (2 or more changes), following Marsden et al. [-@marsden_replication_2018]. 
We noted the nature of the change as one of the following categories (yes/no): 
experimental paradigm, sample, materials/experimental set-up, dependent variable, independent variable, and control. 
We also noted the language under investigation.
The information on whether the article was published open access as well as citation counts and years of publication for both studies were obtained from Web of Science.
An author overlap was attested when at  least one author was a (co-)author on both articles.
When coding the articles, some cases turned up that could not be anticipated when the study was preregistered. When several self-labeled replication studies were mentioned in one article, we chose the first mentioned study for analysis. If there were one independent, but also one or more inner-paper replications, i.e. experiments that replicated previously obtained results from the same article, we chose the independent study for analysis.

### Results
<!--clear and concise, include tables and figures-->

```{r replication_nos, echo=FALSE}

nr_replications <- coded_articles %>% filter(replication == "1") %>% count() # 117 are replications
nr_direct <- coded_articles %>% filter(type_replication == "direct") %>% count() # 8 are replications
prop_direct <- round((nr_direct / nr_replications) * 100, 1)
nr_partial <- coded_articles %>% filter(type_replication == "partial") %>% count() # 42
nr_conceptual <- coded_articles %>% filter(type_replication == "conceptual") %>% count() #66

# what about the remaining 1?
# oops. line 132. coded it as being a replication, but not experimental because it was a replication of a model - need to always also filter for experimental == 1 then (or re-code saying that we didn't have a look at the rest, when it was not experimental)

```

Out of the 210 articles in the subsample, `r nr_replications` were self-claimed replications according to our criteria. The remaining `r 210-nr_replications` mentions were articles that mentioned the term in other contexts or studies that did not specify the concrete aim of replicating an initial study's design or results. 
Out of these replication studies, we categorized `r nr_conceptual` as conceptual, `r nr_partial` as partial, and only `r nr_direct` as direct replications which amounts to `r prop_direct`\% of all coded cases.

```{r success, echo=FALSE}

# How many of the direct replications were independent
independent_reps = coded_articles %>% filter(type_replication == "direct") %>% filter(auth_overlap == "0") %>% count() # 3 independent
independent_rate = (independent_reps / nr_direct) * 100
# --> 37.5%

# How many of these were (self-labeled) successful?
success_reps = coded_articles %>% 
  filter(type_replication == "direct",
         auth_overlap == "0",
         success == "1") %>% 
  count() # 5


```

Looking closer at direct replications, `r independent_reps` studies were independent studies, i.e. there was no overlap between authors of the initial study and the replication study. 
Out of these independent direct replication studies, `r success_reps` were self-labeled as successful replications. 
In other words, our sample included only one independent failed replication attempt.

Figure X illustrates the development of replication studies across the time span of our sample. 
While the overall number of studies increased over the years, the proportion of direct replications remained stable at best. 
However, it seems as if there is an increasing number of partial and conceptual replications that was published within the last few years.^[Given the small number of direct replications in our sample, both a descriptive assessment and an inferential assessment as preregistered are very uninformative. The reader is directed to the supplementary materials, if they are interested in the model outputs of the preregistered analysis.]. 

```{r steam_plot, out.width="100%", fig.align = 'center', fig.asp = 0.7}

# subset data
replication_sub <- coded_articles %>% 
  filter(experimental == 1)

### let's plot the distribution in stream plot
replication_sub_agg <- replication_sub %>% 
  mutate(type2 = ifelse(is.na(type_replication), "no", 
                        ifelse(type_replication == "", "no", type_replication))) %>% 
  group_by(type2, pub_year) %>% 
  summarise(n = n())
  
## change level orfer           
replication_sub_agg$type2 <- factor(replication_sub_agg$type2, 
                                    levels = c("no", "conceptual", "partial", "direct"))

## stream plot
stream_plot <- 
ggplot(replication_sub_agg, aes(x = pub_year, y = n, fill = type2)) +
  geom_stream(bw = 0.7)  +
  scale_fill_manual(values = c("#998ec3", "#fee0b6", "#f1a340", "#b35806"),
                    name = "") +
  labs(title = "Number of replication types from 1988-2020",
       subtitle = "   ",
       x = "\nYear",
       y = "number of papers in corpus\n") +
  theme_minimal() +
  theme(legend.key.height = unit(2,"line"),
        legend.title = element_text(face = "bold", size = 12),
        legend.text = element_text(size = 12),
        strip.background = element_blank(),
        strip.text = element_text(size = 12, face = "bold"),
        panel.spacing = unit(2, "lines"),
        panel.border = element_blank(),
        plot.background = element_rect(fill = "transparent", colour = NA),
        strip.text.y = element_text(size = 12, hjust = 0),
        axis.text = element_text(size = 12),
        axis.line = element_blank(),
        axis.title.y = element_text(size = 10),
        axis.title.x = element_text(size = 12),
        plot.title = element_text(size = 14, face = "bold"),
        plot.margin = unit(c(0.6,0.6,0.6,0.6),"cm"))
stream_plot

```



```{r river plot, echo=FALSE}

myedges <- data.frame(ID = 1:9,
                   N1 = c("successful", "failure", 
                          "author overlap", "independent",
                          "direct", "conceptual", "partial",
                          #"partial",
                          "No replications", "Self-claimed replications"),
                   Value = c(2,1,
                             5,3,
                             8,66,42,
                             94,116))

myedges$N1 <- paste(myedges$N1, "\n", paste0(myedges$Value))
myedges$N2 <- c(rep("independent \n 3", 2), 
                rep("direct \n 8", 2), 
                rep("Self-claimed replications \n 116", 3), 
                rep(paste("All surveyed papers \n", 210), 2))


mynodes <- data.frame(ID=c(myedges$N1, paste("All surveyed papers \n", 210)), 
                      x = c(1, 1, 2, 2, 3, 3, 3, 4, 4, 5), 
                      y = c(10,14,
                            8, 12, 
                            10, 7.5, 5, 
                            1, 7.5, 
                            5))

# styles
palette <- c("gray", "#AA3377", "gray", 
             "#AA3377", "grey", "gray", 
             "gray", "gray", "#AA3377")

#AA3377

mystyles = lapply(mynodes$y, function(n) {
  list(col = palette[n])
})

for(i in 1:length(palette)){
  mystyles[[i]]$col<-palette[i]
}

names(mystyles) <- mynodes$ID

custom.style <- riverplot::default.style()
custom.style$textcex <- 1.8

svg("../../plots/River.svg", width=10)
myriv <- riverplot::makeRiver(nodes=mynodes, edges=myedges, node_styles=mystyles)
plot(myriv, default_style=custom.style, nodewidth=1, plot_area = 0.95)
dev.off()
#dev.off() KK: was in here twice
plot(myriv, default_style=custom.style, nodewidth=1, plot_area=.88)


```

```{r language rep, echo=FALSE}
# How many replication studies were replications in a different language?
reps_diff_lang = coded_articles %>% 
  filter(replication == "1") %>% 
  filter(str_detect(comments, "replication in a different language")) %>% 
  count()
# --> 18
diff_lang = round(((reps_diff_lang / nr_replications) * 100),1)
# --> ~ 15,38 %

# number of english studies across replication
reps_english = coded_articles %>% 
  filter(replication == "1",
         str_detect(language, "English")) %>% 
  count()
         
reps_english_rate = round(((reps_english / nr_replications) * 100),1)

```

One possible reason for the absence of increasing (direct) replication rates could be that experimental linguistics predominantly replicates experimental findings across languages, making it by definition at least only a partial replication.
However, only a minority of replications targeted a different language than the initial study (`r diff_lang`\%). 
The majority of replication efforts were conducted within the same language as the initial study.
In fact, `r reps_english_rate`\% of all replication studies were studies on English varieties.

```{r years between, echo=FALSE}
# What is the median number of years between an initial and a replication study?
median_year_between <- coded_articles %>% 
  filter(cit_init_study != "same") %>% # exclude inner-paper replications
  summarise(median_years = round(median(years_between, na.rm = TRUE),1)) # 7 years

```


```{r citation, echo=FALSE}

coded_articles <- coded_articles %>% 
  mutate(year_til_today = 2020 - pub_year)

# What is the mean and median citation count of initial studies before a replication study was published?
mean_citation_between <- coded_articles %>% 
  filter(cit_init_study != "same",
         replication == 1) %>% # exclude inner-paper replications
  summarise(mean_cit_init = round(mean(init_cit_til_rep, na.rm = TRUE),1)) # ~ 41.1 citations
median_citation_between <- coded_articles %>% 
  filter(cit_init_study != "same",
         replication == 1) %>% # exclude inner-paper replications
  summarise(median_cit_init = round(median(init_cit_til_rep, na.rm = TRUE),1)) # 19.5 citations

mean_citation_rate <- round(mean_citation_between/median_year_between,1)

# What is the mean and median citation count of replication studies 
median_citation_replication <- coded_articles %>% 
  filter(cit_init_study != "same",
         replication == 1) %>% 
  summarise(median_cit_rep = round(median(rep_citation, na.rm = TRUE), 1),
            median_cit_rep_rate = round(median(year_til_today / median_cit_rep), 1))

# jif of journals publishing replication and initial study
median_jif <- df %>% 
  summarise(median_jif = round(median(jif, na.rm = TRUE), 1))

```

The median number of years between an initial and a replication study is `r median_year_between` years.
Initial studies were on average `r mean_citation_between` times cited before a replication was published which amounts to a average yearly citation rate of `r mean_citation_rate` citations. 
This is average citation rate was well above the impact factor of journals in publishing experimental studies in linguistics (median JIF in superset: `r median_jif`). 
However, replications were on average `r median_citation_replication[1]` times cited which amounts to only amounts to an average yearly citation rate (calculated up to the time of analysis) of `r median_citation_replication[2]` citations. 
These numbers are in line with @marsden_replication_2018 investigated the prevalence of replication studies across second language research. 
They found that replication studies were on average conducted after more than six years and over a hundred citations of the original study. 
Thus, replications are either only performed after the original study had already impacted the field substantially or is only then published if the original study was impactful.
The drop in citations for replication studies compared to the initial study is in line with the lack of perceived prestige
of replication studies [e.g., @koole_rewarding_2012; @nosek_scientific_2012].

### Case study of Journal of Memory and Language

```{r jml, echo=FALSE}
# having a closer look at jml
jml_data <- coded_articles %>% 
  filter(journal == "JOURNAL OF MEMORY AND LANGUAGE")

## how many really experimental
jml_exp <- jml_data %>% 
  filter(experimental == "1") 
jml_exp_ratio = count(jml_exp)/count(jml_data) 
# ---> 50 = 100%

## of those how many actual replications
jml_reps <- jml_exp %>% 
  filter(replication == "1")
jml_rep_ratio = count(jml_reps)/count(jml_exp)
# ---> 34 = 68%

## of those what types of replications
round(xtabs(~type_replication, jml_reps) / nrow(jml_reps), 2)
### 0.44 conceptual
### 0.09 direct --> compared to 7% over all journals
### 0.47 partial

## of those author overlap
round(xtabs(~type_replication + auth_overlap, jml_reps) / nrow(jml_reps), 2)
###            no   yes 
### conceptual 0.18 0.26
### direct     0.06 0.03
### partial    0.24 0.24

# all journals:
###            no   yes 
### conceptual 0.26 0.30
### direct     0.03 0.04
### partial    0.15 0.21

## overall direct independent replication rate 
round(nrow(jml_data[jml_data$replication == 1 &
                jml_data$type_replication == "direct" &
                jml_data$auth_overlap == 0,]) / nrow(jml_data), 3)
### 0.04 = 4% compared to 0.015 = 1,5%
```




```{r plot cit and years direct, echo=FALSE, message=FALSE}
# preparing data for plot

# wrangling citation data
cit_data <- coded_articles %>% 
  filter(type_replication == "direct") %>% # only include direct replications
  mutate(
    obs_nr = c(1:8)
  ) %>% 
  select(obs_nr, rep_citation, init_cit_til_rep) %>% 
  pivot_longer(cols = c(rep_citation, init_cit_til_rep), values_to = "citation") %>% 
  mutate(
    study_kind = ifelse(name == "rep_citation", "replication", "initial")
  ) %>% 
  select(obs_nr, study_kind, citation)

# wrangling year of publication data
pub_data <- coded_articles %>% 
  filter(type_replication == "direct") %>% # only include direct replications
  mutate(
    obs_nr = c(1:8)
  ) %>%
  select(obs_nr, pub_year, year_init_study) %>% 
  pivot_longer(cols = c(pub_year, year_init_study), values_to = "years") %>% 
  mutate(
    study_kind = ifelse(name == "pub_year", "replication", "initial")
  ) %>% 
  select(obs_nr, study_kind, years)

# creating one data frame for plotting
plot_data <- full_join(cit_data, pub_data)

# creating a plot
plot_data %>% 
  ggplot(aes(x = years, y = citation, color = study_kind, group = obs_nr)) +
  geom_point() +
  geom_line(color = "lightgrey") +
  labs(
    x = "years",
    y = "citations",
    title = "After how many years and how many citations do replication studies get published?"
  )
```

```{r plot cit and years all, echo=FALSE, message=FALSE, warning=FALSE}
# preparing data for plot

# wrangling citation data
cit_data <- coded_articles %>% 
  filter(replication == "1") %>% # only include replications
  mutate(
    obs_nr = c(1:117)
  ) %>% 
  select(obs_nr, rep_citation, init_cit_til_rep) %>% 
  pivot_longer(cols = c(rep_citation, init_cit_til_rep), values_to = "citation") %>% 
  mutate(
    study_kind = ifelse(name == "rep_citation", "replication", "initial")
  ) %>% 
  select(obs_nr, study_kind, citation)

# wrangling year of publication data
pub_data <- coded_articles %>% 
  filter(replication == "1") %>% # only include replications
  mutate(
    obs_nr = c(1:117)
  ) %>%
  select(obs_nr, pub_year, year_init_study) %>% 
  pivot_longer(cols = c(pub_year, year_init_study), values_to = "years") %>% 
  mutate(
    study_kind = ifelse(name == "pub_year", "replication", "initial")
  ) %>% 
  select(obs_nr, study_kind, years)

# creating one data frame for plotting
plot_data <- full_join(cit_data, pub_data)

# creating a plot
plot_data %>% 
  ggplot(aes(x = years, y = citation, color = study_kind, group = obs_nr)) +
  geom_point() +
  geom_line(color = "lightgrey") +
  labs(
    x = "years",
    y = "citations",
    title = "After how many years and how many citations do replication studies get published?"
  )
```


```{r same journal, echo=FALSE}
### STUFF THAT I'D TOSS

# How many replication studies were published in the same journal?
coded_articles$journal_init_study <- as.factor(coded_articles$journal_init_study)
coded_articles$journal <- as.factor(coded_articles$journal)
journal_init = factor(coded_articles$journal_init_study, levels = levels(coded_articles$journal)) # set levels equal
reps_same_journal = coded_articles %>% 
  filter(journal == journal_init) %>% 
  filter(cit_init_study != "same") %>% # exclude inner-paper studies
  count()

# --> 16 (52 without exclusion)
same_journal = round(((reps_same_journal / nr_replications) * 100),1)
# --> ~ 13,68 %

# Comparing initial studies to their corresponding replication studies, `r same_journal`% of replications are published in the same journal as the initial study. 
```

```{r inner paper, echo=FALSE}
# How many replication studies were inner-paper replications?
reps_inner_paper = coded_articles %>% 
  filter(replication == "1") %>% 
  filter(cit_init_study == "same") %>% 
  count() 
# --> 37
inner_paper = round(((reps_inner_paper / nr_replications) * 100),1)
# --> ~ 31,62 %
```

### Discussion
- too little replication attempts in experimental linguistics
- journals guidelines generally don't encourage replication studies
- ...


# General discussion
- compare rate of replication mention to previous studies in different fields --> broader picture

### Caveats
This procedure is necessarily only a rough proxy of relevant experimental linguistic articles published in the field and several articles might thus have been overlooked and not been included in the analysis.


# Appendices
identified as A, B, etc.


References {#references .unnumbered}
==========
