---
title             : "Replication Studies in Linguistics Journals"
shorttitle        : "Preregistration"

author: 
  - name          : "Kristina Kobrock"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Timo B. Roettger"
    affiliation   : "2"
    role:
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Osnabrueck University"
  - id            : "2"
    institution   : "University of Oslo"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction
After the so-called replication crisis has been called out for various disciplines including psychology (Open Science Community, 2015), economics (Camerer et al., 2016), and the social sciences (Camerer et al., 2018),
<!-- TR:  These text forms are usually tricky and we need to make sure our reader is not immediately alienated, so the term "crisis" is always a candidate to avoid in these texts. How about sth like. "Coordinated efforts to replicate published findings across psychological sciences (Open Science Community, 2015), economics (Camerer et al., 2016), and social sciences (Camerer et al., 2018) have uncovered surprisingly low rates of successful replications."-->
researchers increasingly have to ask themselves whether their scientific discipline is also affected by such an amount of non-reproducible findings in standard literature. 
<!-- TR: We gotta be careful not to conflate replicability and reproducibility-->
<!-- TR: I think our focus is not on whether there is a replication crisis (there obviously is), but whether we replicate at all. Maybe we should immediately do two things here: First navigate this discussion immediately to linguistics, i.e. people are worried that we as a field have the same problem: From one of my papers: "There are raising concerns that a similar state of affairs is true for the field of experimental linguistics because it shares with other disciplines many research practices that have been identified to decrease the replicability of published findings (e.g. Marsden et al. 2018a, Roettger & Baer-Henney 2019, SÃ¶nning & Werner, this issue). Moreover, there is already mounting evidence that published experimental findings cannot be taken at face value (e.g., Chen 2007, Papesh 2015, Nieuwland et al. 2018, Stack et al. 2018, Westbury 2018, among many others)."
Second, what are the reasons for this low success rate? One important point here is an asymmetric incentive system, rewarding novel confirmatory findings over null results and replications.
-->

In order to evaluate the replication rate in linguistics, the present study aims at assessing the frequency and typology of replication studies that are published in a representative sample of linguistic journals. 
The present study consists of two parts: First, we will assess the frequency of self-reported replication attempts across 100 linguistic journals and relate the replication rate to factors related to journal policy, impact factor and publication type. Second, we will assess the type of replication studies (direct, partial, conceptual) published in a subset of 20 journals and relate their frequency to factors related to year of publication, and the citation and publication year of the original study.

# Overview: Rate of Replication Mention
The key dependent variable of the first part of this study is the rate of replication mention for journals relevant to the field of experimental linguistics. In order to determine these rates for the individual journals, we will draw on a method introduced by Makel et al. (2012) who used it for the field of psychology. We will use the search engine "Web of Science" (webofknowledge.com) for journal articles that contain the search term "replicat*" in title, abstract or keywords. The rate of replication mention for each journal is calculated by dividing the number of articles containing the search term by the total number of articles published in the respective journal.
 <!-- TR: the last sentence can be ommitted here, as you describe the process in detail below-->

### Research Questions
We intend to answer the following research questions: How many replication studies have been published in journals representative for experimental linguistic research? How did the rate change over time and does it related to journal policy, impact factor and publication type?

### Sample
To obtain a representative sample of journals relevant for the field of experimental linguistics, we follow the procedure presented here: First, using the Web of Science advanced search on the "Web of Science Core Collection" database, we filter for the category "Linguistics" (WC=(Linguistics)) that lists the articles of every journal covered by Web of Science that was assigned to the subject category of Linguistics (http://images.webofknowledge.com/WOKRS535R111/help/WOS/hp_subject_category_terms_tasca.html). All English language articles from the full available range of complete years (1945-2020) are taken into account. From the resulting set (159.002) only those articles are selected which contain the search term "experiment\*" in their title, abstract or keywords using TS="experiment\*" as a means to filter for experimental linguistic studies resulting in a set of 11.093 articles. The relevant journals are selected based on the obtained article counts. From all journals that include at least one experimental linguistic study (418), journals with less than 100 linguistically related articles (WC=(Linguistics)) are excluded yielding 259 remaining journals. Because we are interested in journals with a high proportion of experimental studies, we calculate the ratio of studies that contained the search term "experiment\*" by the total amount of linguistics related (WC=Linguistics) articles per journal and sort the results in descending order. Our sample constitutes the first 100 journals of that list. See [osf-link] for more details.
 <!-- TR: in the final paper, we will have to add several caveats here, acknowledging that these are all rough proxies and likely overlooking many relevant studies, but we don't need to bother with it now-->

### Procedure
The total number of articles published in a journal and the total number of articles containing the search term "replicat\*" are obtained via Web of Science search for the 100 sampled journals. These numbers serve as a baseline for calculating the rates of replication mention, replicating the method used by Makel et al. (2012). The rates of replication mention are calculated by dividing the number of articles containing the term "replicat*" by the total number of articles for each journal.
 <!-- TR: total number or number of those that have "experiment*"-->

In order to relate the rate of replication mention to journal policies, we further examine the journals' submission guidelines adopting the procedure used by Martin and Clarke (2017). They grouped psychology journals into four classes determined by what was stated in the "instructions to authors" and "aims and scope" sections on the websites of the respective journals:
(1) Journals which stated that they accepted replications;
(2) Journals which did not state they accepted replications but did not discourage replications either;
(3) Journals which implicitly discouraged replications through the use of emphasis on the scientific originality of submissions,
(4) Journals which actively discouraged replications by stating explicitly that they did not accept replications for publication.

 <!-- TR: add information about how the impact factor was extracted and how the open access category is operationalized-->

### Data Analysis
To estimate the rate of replication mention relative to the predictorsjournal impact factors (continuous), open access (binary: open access journal or not) and replication policies (binary: either explicitly encourage or not), we will use Bayesian parameter estimation based on generalized linear regression models with a binomial link function. The model will be fitted to the proportion of replication mentions using the R package brms (Buerkner, 2016). We will use informative normal priors centered on -2.1973 (corresponding to 10% base rate, sd = 5) for the intercept (since we expect very low base rates) and weakly informative Cauchy priors centered on zero (scale = 2.5) for all population-level regression coefficients. These priors are what is referred to as regularizing (Gelman et al., 2008), i.e. our prior assumption is agnostic as to whether the predictors affect the dependent variable, thus making our model conservative with regards to the predictors under investigation. Four sampling chains with 2000 iterations each were run for each model, with a warm-up period of 1000 iterations.
For relevant predictor levels and contrasts between predictor levels, we will report the posterior probability for the rate of replication mention. We summarize these distributions by reporting the posterior mean and the 95% credible intervals (henceforth CrIs, calculated as the highest posterior density interval). 

# Replication Rates and contributing factors

## Methods
The aim of the second part of the present study is to obtain a better understanding of the underlying mechanisms of replication attempts published in the field of experimental linguistics. Because the term "replication" is commonly used in ambiguous ways, the articles that contain the search term "replicat\*" require further analysis to determine whether the articles in question indeed report a replication study or use the term in a different way. We will then be able to refine the rate of replication mention from the first part of the study and instead give a replication rate that captures replication attempts more accurately. 

### Research Questions
The main research questions addressed here are which kinds of replication studies are being published and which factors contribute to their publication. We aim at investigating what types of replication studies are prevalent in the field, i.e. direct, partial or conceptual replications. We are further interested in whether the number of direct citations is related to the number of citations of the initial study and the years between publication of the initial study and the replication attempt.

 <!-- TR: at this level, I think OA and impact factor are irrelevant because we are not looking at journals anymore but at individual papers-->

### Sample
From the superset of 100 journals obtained above, the first 20 journals are selected for a more detailed analysis. We exclude those journals for which less than 2 hits (TS=(replicat\*)) can be obtained. This method yields a total number of 274 articles. Because of the skewed distribution of our sample (114 hits for Journal of Memory and Language, and less than 40 for all other journals), we will randomly select 50 out of the 114 articles for the Journal of Memory and Language to achieve a more balanced distribution of papers across journals.

### Procedure
The sampling procedure above gives us 210 possible self-labeled replication studies. The first step is to identify whether the article indeed presents a replication attempt or not. By reading title and abstract of the paper a first intuition of what the article is about can be obtained. The main task is to investigate whether the authors claim that their underlying aim was to replicate or reproduce findings or methods of an initial study. A search for occurrences of the search term "replicat" in the text and a look at the paragraph before the Methods section and the first paragraph of the Discussion section (following the procedure specified by Makel et al. 2016) helps to further identify the intention communicated by the authors regarding a replication attempt. If the authors communicate that (one of) the underlying aim(s) was to replicate an original study, this article can be treated as a replication. It then qualifies for further analysis after the coding scheme that can be viewed here: [osf-link]. Replication rates are calculated for the 20 journals by correcting the rate of replication mention by the ratio of how many articles including the term "replicat*" have indeed been classified as replications. 

 <!-- TR: this is the "real" replication rate right? I think mentioning this is a little confusing here. Maybe leaving this out or put it later?-->

Under the assumption that the authors did not make any drastic changes to the initial study without reporting them, number and type of changes made by the replication study are assessed. The replication studies are classified according to three types: direct replication (0 changes), partial replication (1 change) and conceptual replication (2 or more changes). We note the nature of the change as one of the following categories: experimental paradigm, sample, materials/experimental set-up, dependent,  independent variable, and control. We will only check whether a change belonging to one of the categories is made (yes/no) and not the power or number of changes.

 <!-- TR: I don't understand this sentence.-->

Coding the articles also involves examining the factors open access (yes/no), years between initial study and replication attempt, author overlap (yes/no), citation counts of both studies and the language under investigation. The information on whether the article is open access can be obtained from Web of Science as well as citation counts and years of publication for both studies. An author overlap is attested to the studies at hand as soon as even one of the authors contributing to one study was also working on the other study.

<!-- TR: The OA information needs to go in the section on part 1. We should also stick to clear terminology for the initial/original study and the replication attempt.-->

### Data Analysis
To estimate the rate of direct replication and its relation to the predictors year of publication (continuous), time lag between publication of initial study and replication attempt (continuous) and number of citations of initial study (continuous), we will use Bayesian parameter estimation based on generalized linear regression models with a logit link function. The model will be fitted to whether the replication mention was a direct replication or not using the R package brms (Buerkner, 2016). The model further includes random intercepts for individual journals.
We will use informative normal priors centered on -2.1973 (corresponding to 10% base rate, sd = 5) for the intercept (since we expect very low base rates) and weakly informative Cauchy priors centered on zero (scale = 2.5) for all population-level regression coefficients. Four sampling chains with 2000 iterations each were run for each model, with a warm-up period of 1000 iterations. For relevant predictor levels and contrasts between predictor levels, we will report the posterior probability for the rate of replication mention. We summarize these distributions by reporting the posterior mean and the 95% credible intervals (henceforth CrIs, calculated as the highest posterior density interval). 
\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
