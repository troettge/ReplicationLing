---
title             : "The title"
shorttitle        : "Title"

author: 
  - name          : "Kristina Kobrock"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Timo B. Roettger"
    affiliation   : "2"
    role:
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Wilhelm-Wundt-University"
  - id            : "2"
    institution   : "Konstanz Business School"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction
After the ”replication crisis” (Zwaan et al., 2018) has been called out for various disciplines including Psychology (Open Science Community, 2015), Economics (Camerer et al., 2016) and the Social Sciences (Camerer et al., 2018), researchers increasingly have to ask themselves whether their scientific discipline is also affected by such an amount of non-reproducible findings in standard literature. This is also true for the field of Experimental Linguistics. Zwaan et al. (2018) argue for ”making replication mainstream” in order to cope with this scientific crisis. Furthermore, they put emphasis on the fact that repeatability is a core property of science that establishes whether a finding is ”scientifically meaningful” (Zwaan et al., 2018). In order to evaluate the replication attempts being currently made in the field of Experimental Linguistics, the here intended study aims to assess the amount of replication studies being published in standard Linguistics journals. Specifically, this paper aims to answer the following research questions: How many replication studies are being published in journals that publish experimental linguistic studies? How did their number change over time? Which factors conribute to replication studies being published in that field?

# Methods
The key dependent variable of the intended study is the replication rate of Experimental Linguistics journals. In order to determine the replication rates of the individual journals, we will draw on a method introduced by Makel et al. (2012) who used it to assess replication rates for journals in the field of Psychology. They searched the search engine "Web of Science" (webofknowledge.com) for articles which contained the search term "replicat*" in the title, abstract or keywords (das sagen die zwar nicht so genau, aber muss ja so sein). The replication rate was then calculated by dividing the number of articles which were replication studies by the total number of articles published in the respective journal. They then analysed a subset of the articles containing the search term in more detail according to 5 features. In order to improve this method and make it applicable to the field of Experimental Linguistics, we modified it according to our purposes drawing also on Marsden et al. (2018) who determined the number of replication studies published in L2 Research.

##  Hypotheses
The two main hypotheses we would like to address with this study are: Even if the number of replication studies will not be higher than in other fields and will probably be in line with the postulated replication crisis, we expect to find a positive trend in the replication rates suggesting that more replication studies are being published today than 10/20 years ago (in line with Makel et al. 2012, ...). We also expect that journals which explicitly encourage replications in their submission guidelines have higher replication rates than journals which do not or even discourage replications implicitly or explicitly. Further we aim to explore the relation between replication rates and journal impact factors, citation counts, (author overlap) and publishing open access.

## Sample
To obtain a representative sample of Experimental Linguistics journals, we follow the procedure presented here: First, using the Advanced Search of Web of Science, we filter for the Web of Science Category "Linguistics" (WC=(Linguistics)) that lists every journal covered by Web of Science that was assigned to the subject category of Linguistics (http://images.webofknowledge.com/WOKRS535R111/help/WOS/hp_subject_category_terms_tasca.html)for the full available range of years (1945-2021) (oder vielleicht nur bis 2020, dann haben wir ein fixed date und obendrein eine runde Zahl mit der wir gut 10-Jahres-Vergleiche machen können?). From this subset we sample only those English language articles which contain the search term "experiment\*" in their title, abstract or keywords using TS="experiment\*" *AND* **LANGUAGE**: (English) *AND* **DOCUMENT TYPES**: (Article). We then calculate the ratio of experimental studies by the total amount of linguistics related articles published in the respective journals and sort them in a descending manner. All journals with less than 100 Linguistics related articles (WC=Linguistics) are excluded. The first 100 journals of the remaining are taken for an overview analysis only including replication rates following Makel et al. (2012) and an assessment of the submission guidelines of these journals following Martin and Clarke (2017). We then take the first 20 from the list for a more detailed analysis of the replication studies, excluding those for which we obtain less than 2 hits running a search for the search term "replicat\*" on titles, abstracts and keywords (TS=(replicat\*)) because they will not be of much use for our later analyses. Please see ... for a full list of (excluded) journals. This method yields a total number of 275 (schon genaue Zahlen nennen?) articles containing the search term "replicat*" which shall be analysed in more detail. Because of the skewed distribution of our sample (116 hits for Journal of Memory and Language, only less than 40 for all other journals), we decided to randomly select 50 out of the 116 articles for the Journal of Memory and Language to make the sample more comparable. 

## Procedure
The total number of articles published in a journal, the total number of experimental studies (articles containing "experiment\*") published in the journal and the total number of articles containing the term "replicat\*" published in the journal are assessed for the 100 journals of our overview sample. These numbers will serve to calculate "experimental rates" and replication rates of the respective journals. We further examine the journals' submission guidelines following the procedure used by Martin and Clarke (2017) who grouped more than one thousand Psychology journals into 4 classes determined by what was stated in the "instructions to authors" and "aims and scope" sections on the websites of the respective journals:
(1) Journals which stated that they accepted replications;
(2) Journals which did not state they accepted replications but did not discourage replications either;
(3) Journals which implicitly discouraged replications through the use of emphasis on the scientific originality of submissions,
(4) Journals which actively discouraged replications by stating explicitly that they did not accept replications for publication.
Because the term "replication" and its different word forms is commonly used in ambiguous ways, the articles require further analysis to determine whether the article indeed reports a replication study or not. We will do this with the smaller subset of 20 journals. By following the sampling procedure described above, we get 209 articles to be analysed. The search procedure brings with it that we can only have a look at self-labelled replication studies. By reading the title and abstract of the paper we are able to get a first intuition of whether the article is indeed a replication attempt or not by checking whether the authors claim that their underlying aim was to replicate/reproduce findings/methods of an original/initial study. We then search for occurrences of the search term "replicat" in the text and have a look at the paragraph before the Methods section and the first paragraph of the Discussion section (following Makel et al. 2016) to check again for the intention communicated by the authors regarding a replication attempt. If the authors communicate that (one of) the underlying aim(s) was to replicate an original/initial study, we can treat this article as a replication and assess in more detail, what type of replication the article presents. While assuming that the authors did not make any severe changes to the original study without reporting them, we assess the number and type of changes made to the initial study and classify the replication studies according to three types: direct replication (0 changes), partial replication (1 change) and conceptual replication (2 or more changes). Please have a look at the coding sheet available at ... for more features the articles are analysed by. (oder muss das alles noch ausformuliert werden?)

## Data analysis
We used `r cite_r("r-references.bib")` for all our analyses.


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
