---
title             : "Preregistration: Replication Studies in Linguistics Journals"
shorttitle        : "Preregistration of Replication Ling"

author: 
  - name          : "Kristina Kobrock"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Timo B. Roettger"
    affiliation   : "2"
    role:
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Osnabrueck University"
  - id            : "2"
    institution   : "University of Oslo"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction
Coordinated efforts to replicate published findings have uncovered surprisingly low rates of successful replications across the psychological sciences [@open_science_collaboration_estimating_2015], economics (Camerer et al. 2016), and social sciences (Camerer et al. 2018). As many research practices of these quantitative disciplines that have been shown to decrease the replicability of findings are shared with experimental linguistics, there are raising concerns about a similarly low number of replication studies conducted and published in this field (e.g. Marsden et al. 2018, Roettger & Baer-Henney 2019). A number of failed replication attempts in various subfields of linguistics like language comprehension (e.g. Papesh 2015) and predictive processing (e.g. Nieuwland et al. 2018) among many others (e.g. Chen 2007, Stack et al. 2018 & Westbury 2018) provide already some evidence for this increasing worry. 
But in order to thoroughly understand and be able to address this problem, it is important to not only evaluate the number of replication attempts conducted in the field, but also reasons and factors contributing to the (presumably low) number. In our eyes, one crucial factor for this phenomenon is the asymmetric incentive system of publication that rewards novel confirmatory findings over replications and null results.

 <!-- TR: refs need to be integrated like above for open science collab, there are a few missing in the bib file, please add and double check: camerer 2018, makel 2016.-->

In order to evaluate the replication rate in linguistics, the present study aims at assessing the frequency and typology of replication studies that are published in a representative sample of linguistic journals. 
The present study consists of two parts: First, we will assess the frequency of self-reported replication attempts across 100 linguistic journals and relate the replication rate to factors related to journal policy, impact factor and publication type. Second, we will assess the type of replication studies (direct, partial, conceptual) published in a subset of 20 journals and relate their frequency to factors related to year of publication, and the citation and publication year of the original study.

# Overview: Rate of Replication Mention
The key dependent variable of the first part of this study is the rate of replication mention for journals relevant to the field of experimental linguistics. In order to determine these rates for the individual journals, we will draw on a method introduced by Makel et al. (2012) who used it for the field of psychology. We will use the search engine "Web of Science" (https://webofknowledge.com) for journal articles that contain the search term "replicat*" in title, abstract or keywords and compute the rate of replication mention.

### Research Questions
We intend to answer the following research questions: How many replication studies have been published in journals representative for experimental linguistic research? How did the rate change over time and how does it relate to journal policy, impact factor and publication type?

### Sample
To obtain a representative sample of journals relevant for the field of experimental linguistics, we follow the procedure presented here: First, using the Web of Science advanced search on the "Web of Science Core Collection" database, we filter for the category "Linguistics" (WC=(Linguistics)) that lists the articles of every journal covered by Web of Science that was assigned to the subject category of Linguistics (http://images.webofknowledge.com/WOKRS535R111/help/WOS/hp_subject_category_terms_tasca.html). All English language articles from the full available range of complete years (1945-2020) are taken into account. From the resulting set (159.002) only those articles are selected which contain the search term "experiment\*" in their title, abstract or keywords using TS="experiment\*" as a means to filter for experimental linguistic studies resulting in a set of 11.093 articles. The relevant journals are selected based on the obtained article counts. From all journals that include at least one experimental linguistic study (418), journals with less than 100 published articles are excluded yielding 259 remaining journals. Because we are interested in journals with a high proportion of experimental studies, we calculate the ratio of studies that contained the search term "experiment\*" by the total amount of articles per journal and sort the results in descending order. Our sample constitutes the first 100 journals of that list. Counts have been obtained on 20th February 2021. See [osf-link] for more details.
 <!-- KK: I realised that WC=Linguistics is a journal category listing every article published in the journal - so no need to talk about "linguistically related" articles-->
 <!-- TR: in the final paper, we will have to add several caveats here, acknowledging that these are all rough proxies and likely overlooking many relevant studies, but we don't need to bother with it now-->

### Procedure
The total number of articles published in a journal and the total number of articles containing the search term "replicat\*" are obtained via Web of Science search for the 100 sampled journals. These numbers serve as a baseline for calculating the rates of replication mention, replicating the method used by Makel et al. (2012). The rates of replication mention are calculated by dividing the number of articles containing the term "replicat*" by the total number of articles for each journal.
 <!-- TR: total number or number of those that have "experiment*"-->

In order to relate the rate of replication mention to journal policies, we further examine the journals' submission guidelines adopting the procedure used by Martin and Clarke (2017). They grouped psychology journals into four classes determined by what was stated in the "instructions to authors" and "aims and scope" sections on the websites of the respective journals:
(1) Journals which stated that they accepted replications;
(2) Journals which did not state they accepted replications but did not discourage replications either;
(3) Journals which implicitly discouraged replications through the use of emphasis on the scientific originality of submissions,
(4) Journals which actively discouraged replications by stating explicitly that they did not accept replications for publication.

Journal impact factors are extracted via Journal Citation Reports (https://jcr.clarivate.com). The 2019 journal impact factors are calculated by dividing the citations in 2019 to items published in 2017 and 2018 by the total number of citable items in 2017 and 2018.
The open access category is assessed via Web of Science. We distinguish three categories: journals which are listed on the Directory of Open Access Journals (DOAJ) ("DOAJ gold"), journals with some articles being published as open access articles ("partial") and journals with no openly accessible articles ("no").

### Data Analysis
To estimate the rate of replication mention relative to the predictors journal impact factors (continuous), open access (binary: open access journal or not), and replication policies (binary: either explicitly encourage or not), we will use Bayesian parameter estimation based on generalized linear regression models with a binomial link function. The model will be fitted to the proportion of replication mentions using the R package brms (Buerkner, 2016). We will use informative normal priors centered on -2.1973 (corresponding to 10% base rate, sd = 5) for the intercept (since we expect very low base rates) and weakly informative Cauchy priors centered on zero (scale = 2.5) for all population-level regression coefficients. These priors are what is referred to as regularizing (Gelman et al., 2008), i.e. our prior assumption is agnostic as to whether the predictors affect the dependent variable, thus making our model conservative with regards to the predictors under investigation. Four sampling chains with 2000 iterations each were run for each model, with a warm-up period of 1000 iterations.
For relevant predictor levels and contrasts between predictor levels, we will report the posterior probability for the rate of replication mention. We summarize these distributions by reporting the posterior mean and the 95% credible intervals (henceforth CrIs, calculated as the highest posterior density interval). 

# Replication Rates and contributing factors

## Methods
The aim of the second part of the present study is to obtain a better understanding of the underlying mechanisms of replication attempts published in the field of experimental linguistics. Because the term "replication" is commonly used in ambiguous ways, the articles that contain the search term "replicat\*" require further analysis to determine whether the articles in question indeed report a replication study or use the term in a different way. We will then be able to refine the rate of replication mention from the first part of the study and instead give a replication rate that captures replication attempts more accurately. 

### Research Questions
The main research questions addressed here are which kinds of replication studies are being published and which factors contribute to their publication. We aim at investigating what types of replication studies are prevalent in the field, i.e. direct, partial or conceptual replications. We are further interested in whether the number of direct citations is related to the number of citations of the initial study and the years between publication of the initial study and the replication attempt.

 <!-- TR: at this level, I think OA and impact factor are irrelevant because we are not looking at journals anymore but at individual papers-->
  <!-- KK: agree on impact factors, but didn't we want to assess open access for the individual articles?-->

### Sample
From the superset of 100 journals obtained above, the first 20 journals are selected for a more detailed analysis. We exclude those journals for which less than 2 hits (TS=(replicat\*)) can be obtained. This method yields a total number of 274 articles. Because of the skewed distribution of our sample (114 hits for Journal of Memory and Language, and less than 40 for all other journals), we will randomly select 50 out of the 114 articles for the Journal of Memory and Language to achieve a more balanced distribution of papers across journals.

### Procedure
The sampling procedure above gives us 210 possible self-labeled replication studies. The first step is to identify whether the article indeed presents a replication attempt or not. By reading title and abstract of the paper a first intuition of what the article is about can be obtained. The main task is to investigate whether the authors claim that their underlying aim was to replicate or reproduce findings or methods of an initial study. A search for occurrences of the search term "replicat" in the text and a look at the paragraph before the Methods section and the first paragraph of the Discussion section (following the procedure specified by Makel et al. 2016) helps to further identify the intention communicated by the authors regarding a replication attempt. If the authors communicate that (one of) the underlying aim(s) was to replicate an original study, this article can be treated as a replication. It then qualifies for further analysis after the coding scheme that can be viewed here: [osf-link]. Replication rates are calculated for the 20 journals by correcting the rate of replication mention by the ratio of how many articles including the term "replicat*" have indeed been classified as replications. 

 <!-- TR: this is the "real" replication rate right? I think mentioning this is a little confusing here. Maybe leaving this out or put it later?-->

Under the assumption that the authors did not make any drastic changes to the initial study without reporting them, number and type of changes made by the replication study are assessed. The replication studies are classified according to three types: direct replication (0 changes), partial replication (1 change) and conceptual replication (2 or more changes). We note the nature of the change as one of the following categories (yes/no): experimental paradigm, sample, materials/experimental set-up, dependent, independent variable, and control.
Coding the articles also involves examining the factors open access (yes/no), years between initial study and replication attempt, author overlap (yes/no), citation counts of both studies and the language under investigation. The information on whether the article is open access can be obtained from Web of Science as well as citation counts and years of publication for both studies. An author overlap is attested to the studies at hand as soon as even one of the authors contributing to one study was also working on the other study.

<!-- TR: The OA information needs to go in the section on part 1. 
KK: Didn't we want to assess open access on both levels (articles and journals)?-->
<!-- TR: We should also stick to clear terminology for the initial/original study and the replication attempt.-->

### Data Analysis
To estimate the rate of direct replication and its relation to the predictors year of publication (continuous), open access (binary: open access article or not), time lag between publication of initial study and replication attempt (continuous) and number of citations of initial study (continuous), we will use Bayesian parameter estimation based on generalized linear regression models with a logit link function. The model will be fitted to whether the replication mention was a direct replication or not using the R package brms (Buerkner, 2016). The model further includes random intercepts for individual journals.
We will use informative normal priors centered on -2.1973 (corresponding to 10% base rate, sd = 5) for the intercept (since we expect very low base rates) and weakly informative Cauchy priors centered on zero (scale = 2.5) for all population-level regression coefficients. Four sampling chains with 2000 iterations each were run for each model, with a warm-up period of 1000 iterations. For relevant predictor levels and contrasts between predictor levels, we will report the posterior probability for the rate of replication mention. We summarize these distributions by reporting the posterior mean and the 95% credible intervals (henceforth CrIs, calculated as the highest posterior density interval). 
<!-- KK: You wrote "henceforth CrIs" already in the first data analysis part, so can't we use the abbreviation here already?-->
\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
